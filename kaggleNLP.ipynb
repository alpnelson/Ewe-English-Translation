{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "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@github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "MODE = 'train'\n",
    "NUM_EPOCHS = 100\n",
    "NUMBER_OF_DATASET = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(number):\n",
    "\n",
    "    english_data = []\n",
    "    with open('jw300.en-ee.en', 'r+b') as file:\n",
    "\n",
    "        lines = file.readlines()\n",
    "        cnt = 1\n",
    "        for i in range(min(number, len(lines))):\n",
    "            english_data.append(lines[i].decode('utf-8').strip())\n",
    "\n",
    "    ewe_data = []\n",
    "    with open('jw300.en-ee.ee', 'r+b') as file:\n",
    "\n",
    "        # ewe=file.read()\n",
    "        lines = file.readlines()\n",
    "        cnt = 1\n",
    "        for i in range(min(number, len(lines))):\n",
    "            ewe_data.append(lines[i].decode('utf-8').strip())\n",
    "        \n",
    "\n",
    "    return english_data,ewe_data\n",
    "    # return english_data,ewe_data÷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('jw300.en-ee.en', 'r+b') as file:\n",
    "\n",
    "#         lines = file.readlines()\n",
    "# lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines:\n",
    "#     line = line.decode('utf-8')\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalize_eng(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n",
    "\n",
    "def normalize_ewe(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "raw_data_en,raw_data_ewe = read_dataset(NUMBER_OF_DATASET)\n",
    "# raw_data_en, raw_data_ewe = list(zip(*raw_data))\n",
    "raw_data_en = [normalize_eng(data) for data in raw_data_en]\n",
    "raw_data_ewe_in = ['<start> ' + normalize_ewe(data) for data in raw_data_ewe]\n",
    "raw_data_ewe_out = [normalize_ewe(data) + ' <end>' for data in raw_data_ewe]\n",
    "\n",
    "print(len(raw_data_ewe_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_data_ewe_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
    "                                                        padding='post')\n",
    "\n",
    "ewe_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "ewe_tokenizer.fit_on_texts(raw_data_ewe_in)\n",
    "ewe_tokenizer.fit_on_texts(raw_data_ewe_out)\n",
    "data_ewe_in = ewe_tokenizer.texts_to_sequences(raw_data_ewe_in)\n",
    "data_ewe_in = tf.keras.preprocessing.sequence.pad_sequences(data_ewe_in,\n",
    "                                                           padding='post')\n",
    "\n",
    "data_ewe_out = ewe_tokenizer.texts_to_sequences(raw_data_ewe_out)\n",
    "data_ewe_out = tf.keras.preprocessing.sequence.pad_sequences(data_ewe_out,\n",
    "                                                            padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 98), dtype=tf.int32, name=None), TensorSpec(shape=(None, 140), dtype=tf.int32, name=None), TensorSpec(shape=(None, 140), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_ewe_in, data_ewe_out))\n",
    "dataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Create the Positional Embedding\"\"\"\n",
    "\n",
    "\n",
    "def positional_encoding(pos, model_size):\n",
    "    \"\"\" Compute positional encoding for a particular position\n",
    "\n",
    "    Args:\n",
    "        pos: position of a token in the sequence\n",
    "        model_size: depth size of the model\n",
    "    \n",
    "    Returns:\n",
    "        The positional encoding for the given token\n",
    "    \"\"\"\n",
    "    PE = np.zeros((1, model_size))\n",
    "    for i in range(model_size):\n",
    "        if i % 2 == 0:\n",
    "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
    "        else:\n",
    "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
    "    return PE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 64)\n",
      "(10000, 98)\n",
      "(10000, 140)\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(data_en[0]), len(data_ewe_in[0]))\n",
    "MODEL_SIZE = 64\n",
    "\n",
    "pes = []\n",
    "for i in range(max_length):\n",
    "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
    "\n",
    "pes = np.concatenate(pes, axis=0)\n",
    "pes = tf.constant(pes, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(pes.shape)\n",
    "print(data_en.shape)\n",
    "print(data_ewe_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.Model):\n",
    "    \"\"\" Class for Multi-Head Attention layer\n",
    "\n",
    "    Attributes:\n",
    "        key_size: d_key in the paper\n",
    "        h: number of attention heads\n",
    "        wq: the Linear layer for Q\n",
    "        wk: the Linear layer for K\n",
    "        wv: the Linear layer for V\n",
    "        wo: the Linear layer for the output\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, h):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.key_size = model_size // h\n",
    "        self.h = h\n",
    "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
    "        self.wo = tf.keras.layers.Dense(model_size)\n",
    "\n",
    "    def call(self, query, value, mask=None):\n",
    "        \"\"\" The forward pass for Multi-Head Attention layer\n",
    "\n",
    "        Args:\n",
    "            query: the Q matrix\n",
    "            value: the V matrix, acts as V and K\n",
    "            mask: mask to filter out unwanted tokens\n",
    "                  - zero mask: mask for padded tokens\n",
    "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
    "        \n",
    "        Returns:\n",
    "            The concatenated context vector\n",
    "            The alignment (attention) vectors of all heads\n",
    "        \"\"\"\n",
    "        # query has shape (batch, query_len, model_size)\n",
    "        # value has shape (batch, value_len, model_size)\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(value)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        # Split matrices for multi-heads attention\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Originally, query has shape (batch, query_len, model_size)\n",
    "        # We need to reshape to (batch, query_len, h, key_size)\n",
    "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
    "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
    "        query = tf.transpose(query, [0, 2, 1, 3])\n",
    "        \n",
    "        # Do the same for key and value\n",
    "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
    "        key = tf.transpose(key, [0, 2, 1, 3])\n",
    "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
    "        value = tf.transpose(value, [0, 2, 1, 3])\n",
    "        \n",
    "        # Compute the dot score\n",
    "        # and divide the score by square root of key_size (as stated in paper)\n",
    "        # (must convert key_size to float32 otherwise an error would occur)\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
    "        # score will have shape of (batch, h, query_len, value_len)\n",
    "        \n",
    "        # Mask out the score if a mask is provided\n",
    "        # There are two types of mask:\n",
    "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
    "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
    "        if mask is not None:\n",
    "            score *= mask\n",
    "\n",
    "            # We want the masked out values to be zeros when applying softmax\n",
    "            # One way to accomplish that is assign them to a very large negative value\n",
    "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
    "        \n",
    "        # Alignment vector: (batch, h, query_len, value_len)\n",
    "        alignment = tf.nn.softmax(score, axis=-1)\n",
    "        \n",
    "        # Context vector: (batch, h, query_len, key_size)\n",
    "        context = tf.matmul(alignment, value)\n",
    "        \n",
    "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
    "        context = tf.transpose(context, [0, 2, 1, 3])\n",
    "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
    "        \n",
    "        # Apply one last full connected layer (WO)\n",
    "        heads = self.wo(context)\n",
    "        \n",
    "        return heads, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Create the Encoder\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Encoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention: array of Multi-Head Attention layers\n",
    "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
    "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "\n",
    "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, sequence, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Encoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        sub_in = embed_out\n",
    "        alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
    "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
    "            sub_out = sub_in + sub_out\n",
    "            sub_out = self.attention_norm[i](sub_out)\n",
    "            \n",
    "            alignments.append(alignment)\n",
    "            ffn_in = sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_in + ffn_out\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            sub_in = ffn_out\n",
    "\n",
    "        return ffn_out, alignments\n",
    "\n",
    "\n",
    "H = 8\n",
    "NUM_LAYERS = 4\n",
    "vocab_size = len(en_tokenizer.word_index) + 1\n",
    "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "print(vocab_size)\n",
    "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
    "encoder_output, _ = encoder(sequence_in)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Decoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
    "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
    "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
    "        attention_mid: array of middle Multi-Head Attention layers\n",
    "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
    "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "\n",
    "        dense: Dense layer to compute final output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Decoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            encoder_output: output of the Encoder (for computing middle attention)\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The bottom alignment (attention) vectors for all layers\n",
    "            The middle alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        bot_sub_in = embed_out\n",
    "        bot_alignments = []\n",
    "        mid_alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # BOTTOM MULTIHEAD SUB LAYER\n",
    "            seq_len = bot_sub_in.shape[1]\n",
    "\n",
    "            if training:\n",
    "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            else:\n",
    "                mask = None\n",
    "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
    "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
    "            bot_sub_out = bot_sub_in + bot_sub_out\n",
    "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
    "            \n",
    "            bot_alignments.append(bot_alignment)\n",
    "\n",
    "            # MIDDLE MULTIHEAD SUB LAYER\n",
    "            mid_sub_in = bot_sub_out\n",
    "\n",
    "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
    "                mid_sub_in, encoder_output, encoder_mask)\n",
    "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
    "            mid_sub_out = mid_sub_out + mid_sub_in\n",
    "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
    "            \n",
    "            mid_alignments.append(mid_alignment)\n",
    "\n",
    "            # FFN\n",
    "            ffn_in = mid_sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_out + ffn_in\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            bot_sub_in = ffn_out\n",
    "\n",
    "        logits = self.dense(ffn_out)\n",
    "\n",
    "        return logits, bot_alignments, mid_alignments\n",
    "\n",
    "\n",
    "vocab_size = len(ewe_tokenizer.word_index) + 1\n",
    "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "\n",
    "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
    "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
    "decoder_output.shape\n",
    "\n",
    "\n",
    "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(targets, logits):\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\" Learning schedule for training the Transformer\n",
    "\n",
    "    Attributes:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        warmup_steps: number of warmup steps at the beginning\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, warmup_steps=4000):\n",
    "        super(WarmupThenDecaySchedule, self).__init__()\n",
    "\n",
    "        self.model_size = model_size\n",
    "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_term = tf.math.rsqrt(step)\n",
    "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(lr,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_source_text=None):\n",
    "    \"\"\" Predict the output sentence for a given input sentence\n",
    "\n",
    "    Args:\n",
    "        test_source_text: input sentence (raw string)\n",
    "    \n",
    "    Returns:\n",
    "        The encoder's attention vectors\n",
    "        The decoder's bottom attention vectors\n",
    "        The decoder's middle attention vectors\n",
    "        The input string array (input sentence split by ' ')\n",
    "        The output string array\n",
    "    \"\"\"\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
    "\n",
    "    de_input = tf.constant(\n",
    "        [[ewe_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
    "\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
    "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
    "        out_words.append(ewe_tokenizer.index_word[new_word.numpy()[0][0]])\n",
    "\n",
    "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
    "        # so we have to add the last predicted word to create a new input sequence\n",
    "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
    "\n",
    "        # TODO: get a nicer constraint for the sequence length!\n",
    "        if out_words[-1] == '<end>':\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))\n",
    "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out):\n",
    "    \"\"\" Execute one training step (forward pass + backward pass)\n",
    "\n",
    "    Args:\n",
    "        source_seq: source sequences\n",
    "        target_seq_in: input target sequences (<start> + ...)\n",
    "        target_seq_out: output target sequences (... + <end>)\n",
    "    \n",
    "    Returns:\n",
    "        The loss value of the current pass\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
    "        # encoder_mask has shape (batch_size, source_len)\n",
    "        # we need to add two more dimensions in between\n",
    "        # to make it broadcastable when computing attention heads\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
    "\n",
    "        decoder_output, _, _ = decoder(\n",
    "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
    "\n",
    "        loss = loss_func(target_seq_out, decoder_output)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.5212 Elapsed time 10.07s\n",
      "Epoch 1 Batch 100 Loss 1.4572 Elapsed time 111.54s\n",
      "Epoch 1 Batch 200 Loss 1.2233 Elapsed time 112.86s\n",
      "Epoch 1 Batch 300 Loss 1.2658 Elapsed time 112.79s\n",
      "RAPE is a growing threat and a woman s best defense is to be aware and prepared .\n",
      "[[167, 9, 6, 665, 1029, 5, 6, 235, 13, 230, 3091, 9, 4, 20, 1334, 5, 1136, 1]]\n",
      ". . . . . . <end>\n",
      "Epoch 2 Batch 0 Loss 1.0888 Elapsed time 20.62s\n",
      "Epoch 2 Batch 100 Loss 0.9756 Elapsed time 111.88s\n",
      "Epoch 2 Batch 200 Loss 0.8400 Elapsed time 113.45s\n",
      "Epoch 2 Batch 300 Loss 0.7870 Elapsed time 121.04s\n",
      "And it is not enough to pray because you think God might exist .\n",
      "[[5, 11, 9, 21, 296, 4, 1298, 98, 15, 363, 65, 418, 1406, 1]]\n",
      "Exception encountered when calling layer \"decoder_1\" (type Decoder).\n",
      "\n",
      "Incompatible shapes: [1,141,64] vs. [140,64] [Op:AddV2]\n",
      "\n",
      "Call arguments received by layer \"decoder_1\" (type Decoder):\n",
      "  • sequence=tf.Tensor(shape=(1, 141), dtype=int64)\n",
      "  • encoder_output=tf.Tensor(shape=(1, 14, 64), dtype=float32)\n",
      "  • training=False\n",
      "  • encoder_mask=None\n",
      "Epoch 3 Batch 0 Loss 1.0309 Elapsed time 24.52s\n",
      "Epoch 3 Batch 100 Loss 0.8234 Elapsed time 126.66s\n",
      "Epoch 3 Batch 200 Loss 0.8436 Elapsed time 117.64s\n",
      "Epoch 3 Batch 300 Loss 0.8389 Elapsed time 114.58s\n",
      "I fought as I ve never fought before and screamed Jehovah s name so loud so many times that the dogs began to bark .\n",
      "[[12, 2209, 16, 12, 511, 154, 2209, 129, 5, 4204, 45, 13, 496, 66, 2355, 66, 61, 140, 8, 2, 7317, 258, 4, 4654, 1]]\n",
      "Exception encountered when calling layer \"decoder_1\" (type Decoder).\n",
      "\n",
      "Incompatible shapes: [1,141,64] vs. [140,64] [Op:AddV2]\n",
      "\n",
      "Call arguments received by layer \"decoder_1\" (type Decoder):\n",
      "  • sequence=tf.Tensor(shape=(1, 141), dtype=int64)\n",
      "  • encoder_output=tf.Tensor(shape=(1, 25, 64), dtype=float32)\n",
      "  • training=False\n",
      "  • encoder_mask=None\n",
      "Epoch 4 Batch 0 Loss 0.9251 Elapsed time 22.72s\n",
      "Epoch 4 Batch 100 Loss 0.7162 Elapsed time 115.17s\n",
      "Epoch 4 Batch 200 Loss 0.7668 Elapsed time 113.72s\n",
      "Epoch 4 Batch 300 Loss 0.8763 Elapsed time 108.63s\n",
      " The father quoted above admitted I shouldn t be calling him a jerk all the time .\n",
      "[[2, 237, 1284, 853, 2213, 12, 5796, 87, 20, 794, 105, 6, 4486, 50, 2, 56, 1]]\n",
      "Exception encountered when calling layer \"decoder_1\" (type Decoder).\n",
      "\n",
      "Incompatible shapes: [1,141,64] vs. [140,64] [Op:AddV2]\n",
      "\n",
      "Call arguments received by layer \"decoder_1\" (type Decoder):\n",
      "  • sequence=tf.Tensor(shape=(1, 141), dtype=int64)\n",
      "  • encoder_output=tf.Tensor(shape=(1, 17, 64), dtype=float32)\n",
      "  • training=False\n",
      "  • encoder_mask=None\n",
      "Epoch 5 Batch 0 Loss 0.7706 Elapsed time 20.98s\n",
      "Epoch 5 Batch 100 Loss 0.6708 Elapsed time 108.72s\n",
      "Epoch 5 Batch 200 Loss 0.6274 Elapsed time 109.72s\n",
      "Epoch 5 Batch 300 Loss 0.7419 Elapsed time 110.13s\n",
      " To reduce the under five child mortality rates of the year by one third .\n",
      "[[4, 2165, 2, 191, 270, 78, 3666, 1998, 3, 2, 104, 26, 34, 476, 1]]\n",
      "Exception encountered when calling layer \"decoder_1\" (type Decoder).\n",
      "\n",
      "Incompatible shapes: [1,141,64] vs. [140,64] [Op:AddV2]\n",
      "\n",
      "Call arguments received by layer \"decoder_1\" (type Decoder):\n",
      "  • sequence=tf.Tensor(shape=(1, 141), dtype=int64)\n",
      "  • encoder_output=tf.Tensor(shape=(1, 15, 64), dtype=float32)\n",
      "  • training=False\n",
      "  • encoder_mask=None\n",
      "Epoch 6 Batch 0 Loss 0.7126 Elapsed time 21.77s\n",
      "Epoch 6 Batch 100 Loss 0.6176 Elapsed time 110.28s\n",
      "Epoch 6 Batch 200 Loss 0.7735 Elapsed time 111.67s\n",
      "Epoch 6 Batch 300 Loss 0.6058 Elapsed time 112.52s\n",
      "The new personality has a strict limit set well short of allowing mental abuse or physical violence .\n",
      "[[2, 91, 1000, 52, 6, 3240, 2460, 401, 128, 1411, 3, 2843, 830, 115, 24, 260, 175, 1]]\n",
      "e e e e e e e e e e e u u u u u o o o o e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e u la e e e e e e e u la e e e e e e e e e e e e e e e u la e u la u la u u u o . <end>\n",
      "Epoch 7 Batch 0 Loss 0.7087 Elapsed time 18.58s\n",
      "Epoch 7 Batch 100 Loss 0.7562 Elapsed time 109.93s\n",
      "Epoch 7 Batch 200 Loss 0.6861 Elapsed time 109.51s\n",
      "Epoch 7 Batch 300 Loss 0.7996 Elapsed time 109.89s\n",
      "Little rain is recycled for more rainfall .\n",
      "[[185, 1255, 9, 4183, 10, 53, 4554, 1]]\n",
      "gake wo e akpa gatɔ nye nu u u u u . <end>\n",
      "Epoch 8 Batch 0 Loss 0.5769 Elapsed time 14.16s\n",
      "Epoch 8 Batch 100 Loss 0.5936 Elapsed time 111.02s\n",
      "Epoch 8 Batch 200 Loss 0.6524 Elapsed time 111.66s\n",
      "Epoch 8 Batch 300 Loss 0.7155 Elapsed time 113.49s\n",
      "This tragedy is compounded by the desperate situation of millions of African children .\n",
      "[[30, 1457, 9, 11133, 26, 2, 2678, 350, 3, 321, 3, 507, 43, 1]]\n",
      "le wo e akpa gatɔ me la wo e akpa gatɔ me . <end>\n",
      "Epoch 9 Batch 0 Loss 0.6645 Elapsed time 14.97s\n",
      "Epoch 9 Batch 100 Loss 0.5378 Elapsed time 114.93s\n",
      "Epoch 9 Batch 200 Loss 0.7337 Elapsed time 115.82s\n",
      "Epoch 9 Batch 300 Loss 0.7745 Elapsed time 117.66s\n",
      "Commenting on Jesus prophetic words in answer to the questions asked by the apostles on the Mount of Olives Professor A .\n",
      "[[3275, 19, 182, 5113, 265, 7, 455, 4, 2, 681, 268, 26, 2, 4033, 19, 2, 3827, 3, 11104, 1031, 6, 1]]\n",
      "le e e e e e e e e e e e e e e e e e e e e e e e e e e e u la dzi . <end>\n",
      "Epoch 10 Batch 0 Loss 0.6992 Elapsed time 16.26s\n",
      "Epoch 10 Batch 100 Loss 0.7982 Elapsed time 116.72s\n",
      "Epoch 10 Batch 200 Loss 0.7860 Elapsed time 118.54s\n",
      "Epoch 10 Batch 300 Loss 0.7318 Elapsed time 118.26s\n",
      "It is worse .\n",
      "[[11, 9, 808, 1]]\n",
      "ame o . <end>\n",
      "Epoch 11 Batch 0 Loss 0.4798 Elapsed time 15.15s\n",
      "Epoch 11 Batch 100 Loss 0.6562 Elapsed time 119.03s\n",
      "Epoch 11 Batch 200 Loss 0.7488 Elapsed time 120.32s\n",
      "Epoch 11 Batch 300 Loss 0.5911 Elapsed time 117.83s\n",
      "The family unit was Jehovah s arrangement for man from the beginning .\n",
      "[[2, 92, 2099, 14, 45, 13, 1750, 10, 124, 28, 2, 1063, 1]]\n",
      "le biblia me la e nya sia me be enye nusi me o . <end>\n",
      "Epoch 12 Batch 0 Loss 0.6725 Elapsed time 15.49s\n",
      "Epoch 12 Batch 100 Loss 0.5868 Elapsed time 118.44s\n",
      "Epoch 12 Batch 200 Loss 0.7279 Elapsed time 117.69s\n",
      "Epoch 12 Batch 300 Loss 0.6405 Elapsed time 119.68s\n",
      "Despite the desperate conditions of a deteriorating world scene Jesus presence in kingly power began over years ago and has been felt in an unmistakable and positive way .\n",
      "[[921, 2, 2678, 933, 3, 6, 2630, 68, 1578, 182, 785, 7, 4031, 438, 258, 96, 81, 479, 5, 52, 62, 402, 7, 37, 11115, 5, 989, 106, 1]]\n",
      "le e e e e e e e e e e e e e e e e e e e e e e e e e e u la nu u u u u u u u u u u u u u u u u u . <end>\n",
      "Epoch 13 Batch 0 Loss 0.5552 Elapsed time 17.28s\n",
      "Epoch 13 Batch 100 Loss 0.6338 Elapsed time 117.77s\n",
      "Epoch 13 Batch 200 Loss 0.6291 Elapsed time 117.93s\n",
      "Epoch 13 Batch 300 Loss 0.5913 Elapsed time 118.50s\n",
      "The girls I liked were horrified .\n",
      "[[2, 475, 12, 2524, 39, 3477, 1]]\n",
      "egblɔ be egble . <end>\n",
      "Epoch 14 Batch 0 Loss 0.5581 Elapsed time 15.06s\n",
      "Epoch 14 Batch 100 Loss 0.4777 Elapsed time 118.20s\n",
      "Epoch 14 Batch 200 Loss 0.6224 Elapsed time 118.47s\n",
      "Epoch 14 Batch 300 Loss 0.5590 Elapsed time 118.72s\n",
      "It is estimated that one out of four teenage girls manifests at least one symptom of an eating disorder most commonly severe dieting .\n",
      "[[11, 9, 840, 8, 34, 76, 3, 272, 956, 475, 7505, 33, 426, 34, 5325, 3, 37, 1281, 4142, 97, 2591, 650, 3435, 1]]\n",
      "le numekuku a e me la nyɔnu siwo gbɔ wodɔ sesee dometɔ eka me be nyɔnuwo e akpa gatɔ me o . <end>\n",
      "Epoch 15 Batch 0 Loss 0.5642 Elapsed time 16.11s\n",
      "Epoch 15 Batch 100 Loss 0.5346 Elapsed time 119.28s\n",
      "Epoch 15 Batch 200 Loss 0.5738 Elapsed time 120.02s\n",
      "Epoch 15 Batch 300 Loss 0.5168 Elapsed time 119.41s\n",
      "You the one expressing abhorrence of the idols do you rob temples ?\n",
      "[[15, 2, 34, 3084, 8838, 3, 2, 6073, 48, 15, 8839, 4638, 25]]\n",
      "e wo e kɔ a atsi sue a e si o atsyɔ nɛ la dzi . <end>\n",
      "Epoch 16 Batch 0 Loss 0.4421 Elapsed time 15.83s\n",
      "Epoch 16 Batch 100 Loss 0.5124 Elapsed time 119.78s\n",
      "Epoch 16 Batch 200 Loss 0.5399 Elapsed time 118.79s\n",
      "Epoch 16 Batch 300 Loss 0.5282 Elapsed time 120.48s\n",
      "This spelled the end for Hitler s final blitzkrieg and his shattered forces began to withdraw .\n",
      "[[30, 10374, 2, 262, 10, 541, 13, 1556, 10375, 5, 35, 3489, 984, 258, 4, 4994, 1]]\n",
      "le mia e a a a a a a a a a a a a a a a a a u si nɔ anyi le la me eye woyi edzi le a a a a a a a a a a a a a a a a e me . <end>\n",
      "Epoch 17 Batch 0 Loss 0.6778 Elapsed time 18.02s\n",
      "Epoch 17 Batch 100 Loss 0.4505 Elapsed time 118.61s\n",
      "Epoch 17 Batch 200 Loss 0.5989 Elapsed time 120.42s\n",
      "Epoch 17 Batch 300 Loss 0.5169 Elapsed time 124.69s\n",
      "Senior abuse An estimated percent of Canada s seniors suffer physical and psychological abuse at the hands of their adult children .\n",
      "[[3550, 115, 37, 840, 172, 3, 630, 13, 6280, 572, 260, 5, 1666, 115, 33, 2, 725, 3, 27, 832, 43, 1]]\n",
      "le ame ge e siwo va yi me la ame tsitsiwo dometɔ a eke meli o . <end>\n",
      "Epoch 18 Batch 0 Loss 0.5947 Elapsed time 16.25s\n",
      "Epoch 18 Batch 100 Loss 0.4502 Elapsed time 121.00s\n",
      "Epoch 18 Batch 200 Loss 0.5896 Elapsed time 120.69s\n",
      "Epoch 18 Batch 300 Loss 0.6379 Elapsed time 120.67s\n",
      "The Arnolds are going to church . \n",
      "[[2, 12758, 18, 264, 4, 117, 1]]\n",
      "mia e a a a a a a a . <end>\n",
      "Epoch 19 Batch 0 Loss 0.5322 Elapsed time 15.58s\n",
      "Epoch 19 Batch 100 Loss 0.4576 Elapsed time 121.54s\n",
      "Epoch 19 Batch 200 Loss 0.5695 Elapsed time 119.92s\n",
      "Epoch 19 Batch 300 Loss 0.5949 Elapsed time 117.06s\n",
      "Or you might simply agree with the criticism .\n",
      "[[24, 15, 418, 277, 1443, 17, 2, 595, 1]]\n",
      "alo hadzidzi ate u ana ame bubuwo ha . <end>\n",
      "Epoch 20 Batch 0 Loss 0.3727 Elapsed time 14.96s\n",
      "Epoch 20 Batch 100 Loss 0.4482 Elapsed time 116.59s\n",
      "Epoch 20 Batch 200 Loss 0.5306 Elapsed time 116.76s\n",
      "Epoch 20 Batch 300 Loss 0.5454 Elapsed time 116.60s\n",
      "By this time you will be feeling better and people will start to compliment you on your appearance .\n",
      "[[26, 30, 56, 15, 40, 20, 874, 284, 5, 64, 40, 958, 4, 5479, 15, 19, 41, 2036, 1]]\n",
      "gake ne e o to vovo na wo la anye nu u u u u u o o o o o i i i i i i i i i i i i . <end>\n",
      "Epoch 21 Batch 0 Loss 0.5112 Elapsed time 16.54s\n",
      "Epoch 21 Batch 100 Loss 0.5576 Elapsed time 116.80s\n",
      "Epoch 21 Batch 200 Loss 0.5081 Elapsed time 116.51s\n",
      "Epoch 21 Batch 300 Loss 0.5397 Elapsed time 117.36s\n",
      "After graduating some of the students take up special pioneer work or traveling work areas of service in which there is a great need in Nigeria .\n",
      "[[93, 3001, 49, 3, 2, 432, 136, 80, 355, 2260, 133, 24, 2283, 133, 602, 3, 403, 7, 101, 67, 9, 6, 345, 157, 7, 985, 1]]\n",
      "le eyi i a e me la ame a ewo siwo me wowɔa dɔ si me wowɔa dɔ le o o o o o o o o o o o si me wowɔa dɔ le la me . <end>\n",
      "Epoch 22 Batch 0 Loss 0.4477 Elapsed time 16.71s\n",
      "Epoch 22 Batch 100 Loss 0.4357 Elapsed time 116.68s\n",
      "Epoch 22 Batch 200 Loss 0.4476 Elapsed time 117.84s\n",
      "Epoch 22 Batch 300 Loss 0.6543 Elapsed time 117.06s\n",
      "Time magazine reports There s an acrid tang bitter taste in nearly every area of modern American pop culture .\n",
      "[[56, 227, 253, 67, 13, 37, 9515, 9516, 4804, 1343, 7, 677, 144, 1091, 3, 686, 443, 3255, 2042, 1]]\n",
      "numekuku a ewo si wowɔ e o o o o o e akunyawɔwɔ e akpa gatɔ e akpa gatɔ u . <end>\n",
      "Epoch 23 Batch 0 Loss 0.3951 Elapsed time 15.65s\n",
      "Epoch 23 Batch 100 Loss 0.4424 Elapsed time 117.58s\n",
      "Epoch 23 Batch 200 Loss 0.4237 Elapsed time 117.75s\n",
      "Epoch 23 Batch 300 Loss 0.5048 Elapsed time 117.28s\n",
      "But there is another reason .\n",
      "[[31, 67, 9, 114, 365, 1]]\n",
      "gake menye eko menye nenema o . <end>\n",
      "Epoch 24 Batch 0 Loss 0.3863 Elapsed time 15.24s\n",
      "Epoch 24 Batch 100 Loss 0.5394 Elapsed time 118.30s\n",
      "Epoch 24 Batch 200 Loss 0.5227 Elapsed time 118.61s\n",
      "Epoch 24 Batch 300 Loss 0.5642 Elapsed time 118.47s\n",
      "Who is best served if the child keeps silent about the abuse ?\n",
      "[[36, 9, 230, 1445, 47, 2, 78, 2333, 2142, 59, 2, 115, 25]]\n",
      "e woate u anye ne e nyametsotso ma le ame a e me ? <end>\n",
      "Epoch 25 Batch 0 Loss 0.3500 Elapsed time 15.55s\n",
      "Epoch 25 Batch 100 Loss 0.4081 Elapsed time 118.57s\n",
      "Epoch 25 Batch 200 Loss 0.4635 Elapsed time 117.84s\n",
      "Epoch 25 Batch 300 Loss 0.4158 Elapsed time 119.01s\n",
      " Box on page \n",
      "[[368, 19, 108]]\n",
      "a aka si le axa <end>\n",
      "Epoch 26 Batch 0 Loss 0.3925 Elapsed time 15.09s\n",
      "Epoch 26 Batch 100 Loss 0.3735 Elapsed time 119.38s\n",
      "Epoch 26 Batch 200 Loss 0.4930 Elapsed time 119.02s\n",
      "Epoch 26 Batch 300 Loss 0.4292 Elapsed time 119.48s\n",
      "Thus even the perfect man Jesus was moved to tears .\n",
      "[[241, 71, 2, 1643, 124, 182, 14, 571, 4, 322, 1]]\n",
      "eyata ame utɔ utɔ utɔ utɔ utɔ utɔ . <end>\n",
      "Epoch 27 Batch 0 Loss 0.4610 Elapsed time 15.34s\n",
      "Epoch 27 Batch 100 Loss 0.3965 Elapsed time 118.92s\n",
      "Epoch 27 Batch 200 Loss 0.5509 Elapsed time 119.43s\n",
      "Epoch 27 Batch 300 Loss 0.4434 Elapsed time 121.08s\n",
      "But I was used to that by now and never wavered .\n",
      "[[31, 12, 14, 213, 4, 8, 26, 100, 5, 154, 7666, 1]]\n",
      "gake ehia be matsɔ ha o o o o o . <end>\n",
      "Epoch 28 Batch 0 Loss 0.4065 Elapsed time 16.00s\n",
      "Epoch 28 Batch 100 Loss 0.3803 Elapsed time 118.53s\n",
      "Epoch 28 Batch 200 Loss 0.4716 Elapsed time 119.15s\n",
      "Epoch 28 Batch 300 Loss 0.4885 Elapsed time 118.04s\n",
      "Many popular songs over the decades have incorporated the spoken word .\n",
      "[[61, 542, 1114, 96, 2, 1138, 23, 9474, 2, 1610, 242, 1]]\n",
      "hekpe e e u la nu sena le hadzidzi siawo me . <end>\n",
      "Epoch 29 Batch 0 Loss 0.4528 Elapsed time 16.21s\n",
      "Epoch 29 Batch 100 Loss 0.3309 Elapsed time 118.85s\n",
      "Epoch 29 Batch 200 Loss 0.4683 Elapsed time 118.35s\n",
      "Epoch 29 Batch 300 Loss 0.4496 Elapsed time 119.62s\n",
      "Mandela is coming ! \n",
      "[[5107, 9, 590, 58]]\n",
      "mandela gbɔna ! <end>\n",
      "Epoch 30 Batch 0 Loss 0.3140 Elapsed time 15.45s\n",
      "Epoch 30 Batch 100 Loss 0.4220 Elapsed time 119.15s\n",
      "Epoch 30 Batch 200 Loss 0.3499 Elapsed time 118.95s\n",
      "Epoch 30 Batch 300 Loss 0.5446 Elapsed time 119.19s\n",
      "Furthermore a Canadian study conducted in different regions of Africa revealed a higher incidence of AIDS among groups not practicing circumcision than among those who do .\n",
      "[[1191, 6, 3190, 183, 2733, 7, 430, 3793, 3, 263, 1034, 6, 778, 6513, 3, 259, 199, 764, 21, 1738, 760, 72, 199, 94, 36, 48, 1]]\n",
      "numekuku siwo wowɔ le afrika e akɔntabubu nu la ee fia be le xexeame kata e lamesehabɔbɔ e lamesehabɔbɔ e lamesehabɔbɔ e lamesehabɔbɔ e mɔnu siwo bɔ o e eviwo u hiv dɔlekuia kabakaba la u . <end>\n",
      "Epoch 31 Batch 0 Loss 0.3980 Elapsed time 17.05s\n",
      "Epoch 31 Batch 100 Loss 0.4067 Elapsed time 118.57s\n",
      "Epoch 31 Batch 200 Loss 0.3658 Elapsed time 119.31s\n",
      "Epoch 31 Batch 300 Loss 0.4255 Elapsed time 119.44s\n",
      "Scientific matters took second place to theology from the very beginning of the Common Era .\n",
      "[[294, 731, 341, 385, 327, 4, 3357, 28, 2, 168, 1063, 3, 2, 400, 2208, 1]]\n",
      "nyate ee omea va nɔ biblia me nyate e si va zu amegbetɔ e utinya nɛ be enye nusi va yi . <end>\n",
      "Epoch 32 Batch 0 Loss 0.3414 Elapsed time 16.13s\n",
      "Epoch 32 Batch 100 Loss 0.4117 Elapsed time 120.52s\n",
      "Epoch 32 Batch 200 Loss 0.4147 Elapsed time 119.30s\n",
      "Epoch 32 Batch 300 Loss 0.3736 Elapsed time 118.20s\n",
      "It is Jehovah s will that we undergo trials for our future benefit .\n",
      "[[11, 9, 45, 13, 40, 8, 46, 2698, 3024, 10, 79, 338, 733, 1]]\n",
      "esia fia be na mawu le se esia e me la miate u a e si le mia si . <end>\n",
      "Epoch 33 Batch 0 Loss 0.4135 Elapsed time 16.64s\n",
      "Epoch 33 Batch 100 Loss 0.3733 Elapsed time 118.91s\n",
      "Epoch 33 Batch 200 Loss 0.4222 Elapsed time 118.40s\n",
      "Epoch 33 Batch 300 Loss 0.3900 Elapsed time 118.03s\n",
      "However it is not in Nazi Germany only that Jehovah s Witnesses have maintained their integrity in the face of death .\n",
      "[[135, 11, 9, 21, 7, 862, 408, 83, 8, 45, 13, 75, 23, 4495, 27, 983, 7, 2, 307, 3, 178, 1]]\n",
      "gake menye nazi germania u u koe yehowa asefowo le a eme o ke bo le wo e akpa bubuwo ha . <end>\n",
      "Epoch 34 Batch 0 Loss 0.3639 Elapsed time 16.40s\n",
      "Epoch 34 Batch 100 Loss 0.3923 Elapsed time 118.85s\n",
      "Epoch 34 Batch 200 Loss 0.3676 Elapsed time 118.63s\n",
      "Epoch 34 Batch 300 Loss 0.3725 Elapsed time 119.97s\n",
      "Eventually she took her life .\n",
      "[[889, 69, 341, 55, 84, 1]]\n",
      "e e e megbe la meva nɔ tsitsim . <end>\n",
      "Epoch 35 Batch 0 Loss 0.3672 Elapsed time 15.37s\n",
      "Epoch 35 Batch 100 Loss 0.4569 Elapsed time 119.71s\n",
      "Epoch 35 Batch 200 Loss 0.4319 Elapsed time 120.01s\n",
      "Epoch 35 Batch 300 Loss 0.5373 Elapsed time 118.81s\n",
      "Many escaped through the rear exit or out the windows onto a narrow ledge .\n",
      "[[61, 4434, 169, 2, 4397, 5030, 24, 76, 2, 3072, 2335, 6, 2657, 6483, 1]]\n",
      "ame bubu si xɔ e la nɔa dzodzom le xɔa me alo e e e utila la me . <end>\n",
      "Epoch 36 Batch 0 Loss 0.3852 Elapsed time 15.84s\n",
      "Epoch 36 Batch 100 Loss 0.4027 Elapsed time 118.97s\n",
      "Epoch 36 Batch 200 Loss 0.3813 Elapsed time 119.76s\n",
      "Epoch 36 Batch 300 Loss 0.4175 Elapsed time 119.87s\n",
      "Chapter of the book Questions Young People Ask Answers That Work offers a number of helpful suggestions .\n",
      "[[3954, 3, 2, 192, 681, 131, 64, 257, 1410, 8, 133, 2985, 6, 206, 3, 1510, 1974, 1]]\n",
      "agbale si nye questions young people ask e nyati si nye detsi onu akpe e mia e subɔla ene la uti . <end>\n",
      "Epoch 37 Batch 0 Loss 0.3608 Elapsed time 16.83s\n",
      "Epoch 37 Batch 100 Loss 0.3366 Elapsed time 120.89s\n",
      "Epoch 37 Batch 200 Loss 0.5039 Elapsed time 119.86s\n",
      "Epoch 37 Batch 300 Loss 0.4001 Elapsed time 119.68s\n",
      "Instructors Isaiah Mnwe and Pius Oparaocha\n",
      "[[3380, 887, 7268, 5, 5219, 7269]]\n",
      "nufiala siwo nye isaiah mnwe kple pius oparaocha <end>\n",
      "Epoch 38 Batch 0 Loss 0.3690 Elapsed time 15.63s\n",
      "Epoch 38 Batch 100 Loss 0.3309 Elapsed time 119.23s\n",
      "Epoch 38 Batch 200 Loss 0.3726 Elapsed time 118.76s\n",
      "Epoch 38 Batch 300 Loss 0.3662 Elapsed time 118.60s\n",
      "However the earth will continue to be inhabited but not by a crooked and twisted generation .\n",
      "[[135, 2, 195, 40, 557, 4, 20, 3799, 31, 21, 26, 6, 4662, 5, 3229, 1740, 1]]\n",
      "gake ame bubuwo ha le anyigba la gblem le wo e dzitsinya . <end>\n",
      "Epoch 39 Batch 0 Loss 0.3151 Elapsed time 15.76s\n",
      "Epoch 39 Batch 100 Loss 0.3330 Elapsed time 118.81s\n",
      "Epoch 39 Batch 200 Loss 0.3465 Elapsed time 118.81s\n",
      "Epoch 39 Batch 300 Loss 0.3647 Elapsed time 120.47s\n",
      "The other specialist disagreed .\n",
      "[[2, 77, 4206, 7647, 1]]\n",
      "kudɔdala evelia . <end>\n",
      "Epoch 40 Batch 0 Loss 0.3324 Elapsed time 16.28s\n",
      "Epoch 40 Batch 100 Loss 0.3164 Elapsed time 117.70s\n",
      "Epoch 40 Batch 200 Loss 0.4256 Elapsed time 117.22s\n",
      "Epoch 40 Batch 300 Loss 0.3451 Elapsed time 119.41s\n",
      "The clergyman offers a Compact Mini Minute Worship Service that he claims will give him time to deliver a sermon lead hymn singing read Scriptures say prayers and have his congregation out the door . \n",
      "[[2, 3966, 2985, 6, 12669, 12670, 1780, 477, 403, 8, 29, 1222, 40, 200, 105, 56, 4, 2601, 6, 7311, 599, 12671, 3569, 356, 777, 170, 923, 5, 23, 35, 395, 76, 2, 375, 1]]\n",
      "nyɔnuvi a e si kɔe nye biae be nyɔnuvi a e si kɔe nye nyɔnuvi a e gblɔ be edo kpo nu tso sɔlemeha la me eye nunɔla a a a a a e koe tsɔa akplo si kɔkɔ a la da tu kɔwo da tu ye me nyawo gɔme kpɔ o . <end>\n",
      "Epoch 41 Batch 0 Loss 0.3345 Elapsed time 17.75s\n",
      "Epoch 41 Batch 100 Loss 0.3463 Elapsed time 119.38s\n",
      "Epoch 41 Batch 200 Loss 0.3702 Elapsed time 118.85s\n",
      "Epoch 41 Batch 300 Loss 0.4073 Elapsed time 121.55s\n",
      "Stepfathers try to exert some discipline and the girls fight back . \n",
      "[[4055, 434, 4, 11949, 49, 383, 5, 2, 475, 1109, 202, 1]]\n",
      "vifofo yeyeawo adze agbagba be yewoadze agla . <end>\n",
      "Epoch 42 Batch 0 Loss 0.2691 Elapsed time 16.19s\n",
      "Epoch 42 Batch 100 Loss 0.3412 Elapsed time 119.44s\n",
      "Epoch 42 Batch 200 Loss 0.3905 Elapsed time 119.17s\n",
      "Epoch 42 Batch 300 Loss 0.3731 Elapsed time 120.09s\n",
      " THE SOCIETY FOR THE PREVENTION OF CRUELTY TO CHILDREN .\n",
      "[[2, 224, 10, 2, 1235, 3, 3522, 4, 43, 1]]\n",
      "uta sese le eviwo u nu tsitsi e tsitsi u . <end>\n",
      "Epoch 43 Batch 0 Loss 0.2530 Elapsed time 15.55s\n",
      "Epoch 43 Batch 100 Loss 0.3748 Elapsed time 118.78s\n",
      "Epoch 43 Batch 200 Loss 0.3639 Elapsed time 119.66s\n",
      "Epoch 43 Batch 300 Loss 0.3159 Elapsed time 119.60s\n",
      "According to scientists the time is ripe for a flu epidemic similar to the one of that killed from million to million people .\n",
      "[[174, 4, 668, 2, 56, 9, 12126, 10, 6, 2555, 1967, 534, 4, 2, 34, 3, 8, 581, 28, 164, 4, 164, 64, 1]]\n",
      "dzɔdzɔme utinunyalawo gblɔ be eyi i didi a e li si sɔ kple vɔ i didi vevie be woana woa e e e e sia me . <end>\n",
      "Epoch 44 Batch 0 Loss 0.2772 Elapsed time 16.74s\n",
      "Epoch 44 Batch 100 Loss 0.2577 Elapsed time 119.09s\n",
      "Epoch 44 Batch 200 Loss 0.2952 Elapsed time 120.10s\n",
      "Epoch 44 Batch 300 Loss 0.4671 Elapsed time 120.50s\n",
      "Remember Your Real Needs\n",
      "[[487, 41, 535, 423]]\n",
      "wɔ dɔ e mia dzi <end>\n",
      "Epoch 45 Batch 0 Loss 0.2850 Elapsed time 15.69s\n",
      "Epoch 45 Batch 100 Loss 0.3059 Elapsed time 119.57s\n",
      "Epoch 45 Batch 200 Loss 0.3138 Elapsed time 121.30s\n",
      "Epoch 45 Batch 300 Loss 0.3506 Elapsed time 121.21s\n",
      "Behavioral limits for adults One abuser claimed that he had simply lost control and beat his wife .\n",
      "[[4613, 2045, 10, 828, 34, 724, 1657, 8, 29, 38, 277, 568, 221, 5, 851, 35, 184, 1]]\n",
      "nazi dzi u umegawo gblɔ be srɔtɔwo e srɔa u eye wobia be wogblɔe le ye gbɔ sesee la ma o . <end>\n",
      "Epoch 46 Batch 0 Loss 0.4200 Elapsed time 16.69s\n",
      "Epoch 46 Batch 100 Loss 0.3311 Elapsed time 120.18s\n",
      "Epoch 46 Batch 200 Loss 0.3540 Elapsed time 120.49s\n",
      "Epoch 46 Batch 300 Loss 0.4268 Elapsed time 120.01s\n",
      "Seventeen year old Sandrine adds I spend from two to three hours a night on my homework plus the weekends . \n",
      "[[3416, 104, 150, 6601, 1171, 12, 915, 28, 107, 4, 179, 439, 6, 380, 19, 32, 2197, 2312, 2, 2909, 1]]\n",
      "sandrine si xɔ e la gblɔ be ewuivi a e si kɔe nye newsweek to radio la gblɔ be ewuivi siwo xɔ abi la me eye ewuivi la dze egɔme . <end>\n",
      "Epoch 47 Batch 0 Loss 0.3324 Elapsed time 17.42s\n",
      "Epoch 47 Batch 100 Loss 0.3168 Elapsed time 120.70s\n",
      "Epoch 47 Batch 200 Loss 0.3311 Elapsed time 120.33s\n",
      "Epoch 47 Batch 300 Loss 0.3341 Elapsed time 120.30s\n",
      "Within a week the wind and the waves had broken the huge ship into four pieces .\n",
      "[[364, 6, 766, 2, 1487, 5, 2, 7099, 38, 1060, 2, 3223, 3977, 85, 272, 1718, 1]]\n",
      "le esi megbe la eye sa agaxɔmenɔlaawo e e ga a e e asi le edzi zi eka eka ie wu . <end>\n",
      "Epoch 48 Batch 0 Loss 0.2954 Elapsed time 16.99s\n",
      "Epoch 48 Batch 100 Loss 0.3565 Elapsed time 119.81s\n",
      "Epoch 48 Batch 200 Loss 0.4398 Elapsed time 121.23s\n",
      "Epoch 48 Batch 300 Loss 0.3226 Elapsed time 119.85s\n",
      " Well what happens if you don t ?\n",
      "[[128, 51, 1393, 47, 15, 207, 87, 25]]\n",
      "ke ne nyahehea te e uwo ? <end>\n",
      "Epoch 49 Batch 0 Loss 0.3026 Elapsed time 15.54s\n",
      "Epoch 49 Batch 100 Loss 0.3563 Elapsed time 119.60s\n",
      "Epoch 49 Batch 200 Loss 0.4265 Elapsed time 120.81s\n",
      "Epoch 49 Batch 300 Loss 0.3608 Elapsed time 119.67s\n",
      "Isolation rigidity and obsessive secrecy these unhealthy unscriptural attitudes are trademarks of the abusive household .\n",
      "[[4769, 12978, 5, 12979, 3332, 82, 2673, 3381, 2015, 18, 12980, 3, 2, 903, 1788, 1]]\n",
      "ame e nya si ana ame e akpa a e e si dzi nya gbegble kple ame a e si dzi nya gbegble gbɔgblɔ fia le ame o o o o o o o o o o o o o o o o o o o kple nuvlowɔwɔ e usekpɔ eamedzi e nunya me . <end>\n",
      "Epoch 50 Batch 0 Loss 0.2906 Elapsed time 17.92s\n",
      "Epoch 50 Batch 100 Loss 0.2845 Elapsed time 119.15s\n",
      "Epoch 50 Batch 200 Loss 0.3167 Elapsed time 119.71s\n",
      "Epoch 50 Batch 300 Loss 0.3977 Elapsed time 120.25s\n",
      "He wanted to know what the problem was .\n",
      "[[29, 605, 4, 188, 51, 2, 245, 14, 1]]\n",
      "edi . <end>\n",
      "Epoch 51 Batch 0 Loss 0.2788 Elapsed time 16.43s\n",
      "Epoch 51 Batch 100 Loss 0.3235 Elapsed time 121.21s\n",
      "Epoch 51 Batch 200 Loss 0.4158 Elapsed time 117.97s\n",
      "Epoch 51 Batch 300 Loss 0.3150 Elapsed time 120.58s\n",
      "A man behind him shouted Heil Hitler ! \n",
      "[[6, 124, 751, 105, 1817, 1581, 541, 58]]\n",
      "utsu a e gblɔ be heil hitler ! <end>\n",
      "Epoch 52 Batch 0 Loss 0.3591 Elapsed time 16.99s\n",
      "Epoch 52 Batch 100 Loss 0.4181 Elapsed time 120.65s\n",
      "Epoch 52 Batch 200 Loss 0.3072 Elapsed time 120.27s\n",
      "Epoch 52 Batch 300 Loss 0.3310 Elapsed time 118.45s\n",
      "From ancient times God s people have been encouraged to be liberal ready to share . \n",
      "[[28, 620, 140, 65, 13, 64, 23, 62, 1258, 4, 20, 3363, 957, 4, 799, 1]]\n",
      "tso blema hela ɔ lɔawo nu o owo okui e susu me be woatsɔ ade wo e dzixɔsewo ta . <end>\n",
      "Epoch 53 Batch 0 Loss 0.2782 Elapsed time 16.11s\n",
      "Epoch 53 Batch 100 Loss 0.2650 Elapsed time 119.51s\n",
      "Epoch 53 Batch 200 Loss 0.2962 Elapsed time 118.75s\n",
      "Epoch 53 Batch 300 Loss 0.4073 Elapsed time 119.11s\n",
      "In that case do not berate yourself that you let him rape you said Robin Warshaw in I Never Called It Rape .\n",
      "[[7, 8, 373, 48, 21, 4481, 290, 8, 15, 232, 105, 167, 15, 88, 4911, 4912, 7, 12, 154, 216, 11, 167, 1]]\n",
      "ne miegblɔ va lɔ be nafia evi be yele be yeanya nu tso gbɔdɔdɔ le gbɔdɔdɔ me o eye yele egbɔ . <end>\n",
      "Epoch 54 Batch 0 Loss 0.2478 Elapsed time 16.72s\n",
      "Epoch 54 Batch 100 Loss 0.2400 Elapsed time 118.77s\n",
      "Epoch 54 Batch 200 Loss 0.2334 Elapsed time 119.37s\n",
      "Epoch 54 Batch 300 Loss 0.4038 Elapsed time 117.05s\n",
      "While women work their fingers to the bone washing cleaning fixing meals and caring for children in the United States many men enjoy time spent in hanging around says the book The Changing American Family and Public Policy .\n",
      "[[163, 102, 133, 27, 7165, 4, 2, 2412, 2665, 1834, 11845, 2361, 5, 2100, 10, 43, 7, 2, 148, 120, 61, 142, 1053, 56, 972, 7, 3560, 352, 111, 2, 192, 2, 1653, 443, 92, 5, 527, 1986, 1]]\n",
      "togbɔ be nyɔnuwo mate u axe mɔ na afrika o nu le afrika e agbalevi si nye evigbɔdɔla siwo me tɔ esia e si nye evigbɔdɔla le wonju kpɔ la dometɔ eka si me gblɔ be yewoawɔ nuvevi wɔwɔ . <end>\n",
      "Epoch 55 Batch 0 Loss 0.2577 Elapsed time 16.33s\n",
      "Epoch 55 Batch 100 Loss 0.3225 Elapsed time 113.86s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (source_seq, target_seq_in, target_seq_out) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m----> 6\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtarget_seq_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Loss \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m Elapsed time \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     10\u001b[0m                 e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, batch, loss\u001b[38;5;241m.\u001b[39mnumpy(), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m starttime))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "starttime = time.time()\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out)\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
    "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
    "            starttime = time.time()\n",
    "\n",
    "    try:\n",
    "        predict()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = (\n",
    "\n",
    "   'What kind of wrong activities are common where you live, and how are you and your family affected?',\n",
    "    'How are you benefiting from an evening set aside for family worship or personal study?',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'He asked the graduating class: “How are you going to view yourselves as you go to your missionary assignment?',\n",
    "    \"How Are You Benefiting From the New Meeting Format and the Workbook?: (5 min.) Discussion.\",\n",
    "    \"Just how are you recommending yourself to others in this regard?\",\n",
    "    \"“Serpents, offspring of vipers,” he says, “how are you to flee from the judgment of Gehenna?”\",\n",
    "    \"Ask, ‘How are you?’\",\n",
    "    \"How Are You Affected by God’s Dignity and Splendor?\",\n",
    "    \"By the way, how are you doing on time?\",\n",
    "    \"How Are You Affected?\",\n",
    "    \"How are you treating the gift that God has given you?\",\n",
    "    \"How are you involved in the issue that Satan raised regarding Job?\",\n",
    "    \"How are you personally affected?\",\n",
    "    \"All the information needed to repeat a phrase like “How are you doing?”\",\n",
    "    \"Ask, ‘How are you?’\",\n",
    "    'What a ridiculous concept!',\n",
    "    'Your idea is not entirely crazy.',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'What he did is very wrong.',\n",
    "    \"All three of you need to do that.\",\n",
    "    \"Are you giving me another chance?\",\n",
    "    \"Both Tom and Mary work as models.\",\n",
    "    \"Can I have a few minutes, please?\",\n",
    "    \"Could you close the door, please?\",\n",
    "    \"Did you plant pumpkins this year?\",\n",
    "    \"Do you ever study in the library?\",\n",
    "    \"Don't be deceived by appearances.\",\n",
    "    \"Excuse me. Can you speak English?\",\n",
    "    \"Few people know the true meaning.\",\n",
    "    \"Germany produced many scientists.\",\n",
    "    \"Guess whose birthday it is today.\",\n",
    "    \"He acted like he owned the place.\",\n",
    "    \"Honesty will pay in the long run.\",\n",
    "    \"How do we know this isn't a trap?\",\n",
    "    \"I can't believe you're giving up.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bible s Viewpoint\n",
      "[[2, 74, 13, 1120]]\n",
      "biblia e nukpɔsusu <end>\n",
      "\n",
      " The Rod of Discipline Is It Out of Date ?\n",
      "[[2, 1049, 3, 383, 9, 11, 76, 3, 902, 25]]\n",
      "ame oti la ’ <end>\n",
      "\n",
      " Foolishness is tied up with the heart of a boy the rod of discipline is what will remove it far from him . Proverbs .\n",
      "[[2896, 9, 1876, 80, 17, 2, 337, 3, 6, 419, 2, 1049, 3, 383, 9, 51, 40, 2291, 11, 313, 28, 105, 1, 420, 1]]\n",
      "ne e e e la ame oti la va li . lododowo . <end>\n",
      "\n",
      " Any physical punishment is emotionally abusive and should not be sanctioned . Parents Anonymous .\n",
      "[[121, 260, 1190, 9, 1716, 903, 5, 119, 21, 20, 7357, 1, 89, 4078, 1]]\n",
      "ame a ewo e seselelame e utete u eye esia e si le ame o o o o o la nye ame ge e dzidzi edzi . <end>\n",
      "\n",
      "THE BIBLE S mention of the rod of discipline ignites fiery controversy .\n",
      "[[2, 74, 13, 2065, 3, 2, 1049, 3, 383, 7358, 5248, 4079, 1]]\n",
      "ame oti anyi . <end>\n",
      "\n",
      "This is understandable for each year thousands of children die as a direct result of physical abuse by a parent .\n",
      "[[30, 9, 5249, 10, 147, 104, 470, 3, 43, 331, 16, 6, 2292, 362, 3, 260, 115, 26, 6, 314, 1]]\n",
      "esia e fu siwo gblea uta le wo okui u la u eye wogblea eyi i ge e le wo e xexleme dzi akpɔtɔ zi ge e . <end>\n",
      "\n",
      "Perhaps this is why one Bible commentary passes off the Bible s sanction of physical punishment as merely a culturally conditioned opinion . \n",
      "[[447, 30, 9, 122, 34, 74, 5250, 2897, 306, 2, 74, 13, 7359, 3, 260, 1190, 16, 873, 6, 7360, 4080, 2066, 1]]\n",
      "ewohi biblia la o nu tso esi nye biblia e nukpɔsusu de dzesii be kristotɔwo e nukpɔsusu sia nye nukpɔsusu si me biblia nusɔsrɔ si wonaa esiawo o . <end>\n",
      "\n",
      "But cultural opinions did not inspire the Bible God did .\n",
      "[[31, 2293, 5251, 112, 21, 3391, 2, 74, 65, 112, 1]]\n",
      "gake menye kristo e gɔnɔla la o amesiwo o wo o wo o . <end>\n",
      "\n",
      "Are its comments on the rod of discipline unreasonable ?\n",
      "[[18, 110, 1717, 19, 2, 1049, 3, 383, 2898, 25]]\n",
      "e nunya si le ame oti u ? <end>\n",
      "\n",
      "It is important that we examine the rod in its context .\n",
      "[[11, 9, 323, 8, 46, 1462, 2, 1049, 7, 110, 2559, 1]]\n",
      "ele be miadzro nusi to vovo na ame oti fia . <end>\n",
      "\n",
      "To illustrate The individual pieces of a jigsaw puzzle make little sense .\n",
      "[[4, 2294, 2, 1121, 1718, 3, 6, 7361, 4081, 116, 185, 825, 1]]\n",
      "le kpɔ e u me womate u a o . <end>\n",
      "\n",
      "It is only after fitting them together that one can see the whole picture .\n",
      "[[11, 9, 83, 93, 5252, 60, 435, 8, 34, 44, 151, 2, 485, 203, 1]]\n",
      "gake ne ha la ati la ya la ya eka koe wogblea ka le kekea me . <end>\n",
      "\n",
      "Likewise the rod is just one piece of the puzzle .\n",
      "[[947, 2, 1049, 9, 99, 34, 1371, 3, 2, 4081, 1]]\n",
      "nenema gbegbe ame oti nye sia . <end>\n",
      "\n",
      "To see the full picture we must fit the rod in with other Bible principles related to discipline .\n",
      "[[4, 151, 2, 501, 203, 46, 152, 2560, 2, 1049, 7, 17, 77, 74, 436, 1463, 4, 383, 1]]\n",
      "be miase ame oti biblia la gɔme bliboe ate u akpɔ eta le biblia me gɔme osewo nu vɔ ivɔ i kple via e gɔme . <end>\n",
      "\n",
      "A Balanced View\n",
      "[[6, 1264, 271]]\n",
      "nukpɔsusu sia <end>\n",
      "\n",
      "Does the Bible endorse only physical punishment ?\n",
      "[[123, 2, 74, 7362, 83, 260, 1190, 25]]\n",
      "ame o o o o o esia e tso ame ua ? <end>\n",
      "\n",
      "Consider the following advice \n",
      "[[471, 2, 461, 707]]\n",
      "de ugble le a a u o o o siwo gbɔna <end>\n",
      "\n",
      " Never drive your children to resentment . \n",
      "[[154, 1877, 41, 43, 4, 5253, 1]]\n",
      "mega o nya u be viwowo e gbɔdɔdɔ u . <end>\n",
      "\n",
      " Don t over correct your children or you will take all the heart out of them . \n",
      "[[207, 87, 96, 1265, 41, 43, 24, 15, 40, 136, 50, 2, 337, 76, 3, 60, 1]]\n",
      "mega e viwowo e viwowo u o ne menye nenema o ha e eko ke bo . <end>\n",
      "\n",
      " That s much more reasonable than the Bible s advice some may say .\n",
      "[[8, 13, 137, 53, 2067, 72, 2, 74, 13, 707, 49, 54, 170, 1]]\n",
      "ame a ewo ate u a u a u esia i be biblia e nukpɔsusu sia ate u a u a o . <end>\n",
      "\n",
      "But this is the Bible s advice .\n",
      "[[31, 30, 9, 2, 74, 13, 707, 1]]\n",
      "gake biblia e a a u o nu tso mawu gbɔ . <end>\n",
      "\n",
      "It is recorded at Ephesians The New Jerusalem Bible and Colossians Phillips .\n",
      "[[11, 9, 1878, 33, 729, 2, 91, 3392, 74, 5, 1879, 5254, 1]]\n",
      "wo o nu tso biblia e eme eye biblia la de dzesii be mi kata . <end>\n",
      "\n",
      "Yes the Bible s viewpoint is reasonable .\n",
      "[[524, 2, 74, 13, 1120, 9, 2067, 1]]\n",
      "e biblia e nukpɔsusu si le . <end>\n",
      "\n",
      "It acknowledges that physical punishment is usually not the most effective teaching method .\n",
      "[[11, 5255, 8, 260, 1190, 9, 332, 21, 2, 97, 826, 708, 1050, 1]]\n",
      "elɔ zi ge e be menye ame o o o o o ke bo tututue wona noe ha e edzi . <end>\n",
      "\n",
      "Proverbs says Listen to discipline not Feel discipline . \n",
      "[[420, 111, 509, 4, 383, 21, 186, 383, 1]]\n",
      "lododowo gblɔ be o o to amehehe si le gbɔgblɔ me be gbɔgblɔ me o la u . <end>\n",
      "\n",
      "And Proverbs points out that a rebuke works deeper in one having understanding than striking a stupid one a hundred times . \n",
      "[[5, 420, 1464, 76, 8, 6, 5256, 1372, 3393, 7, 34, 295, 1719, 72, 5257, 6, 2899, 34, 6, 827, 140, 1]]\n",
      "eye lododowo a i ha gblɔ be lododowo paulo e nukpɔsusu sia tɔgbe tɔgbe si li la me . <end>\n",
      "\n",
      "Furthermore Deuteronomy recommends preventive discipline taking advantage of casual moments to instill moral values in one s children .\n",
      "[[1191, 1587, 2900, 7363, 383, 510, 1373, 3, 4082, 2901, 4, 3394, 761, 1880, 7, 34, 13, 43, 1]]\n",
      "hekpe e u be woaza mate u a e vi siwo wona nui u a o la udɔ atsɔ akpɔ egbɔe a e si me tɔwo e nuwɔwɔ e nuhiahia siwo gbɔna la uti . <end>\n",
      "\n",
      "Thus the Bible s view of discipline is balanced .\n",
      "[[241, 2, 74, 13, 271, 3, 383, 9, 1264, 1]]\n",
      "eyata biblia e nukpɔsusu sia fia alesi biblia la nye nusi o kpe . <end>\n",
      "\n",
      "What About the Rod ?\n",
      "[[51, 59, 2, 1049, 25]]\n",
      "ke ame oti ? <end>\n",
      "\n",
      "Still the Bible does mention the rod of discipline .\n",
      "[[173, 2, 74, 123, 2065, 2, 1049, 3, 383, 1]]\n",
      "ke ha la biblia la o nu tso ati si li . <end>\n",
      "\n",
      "How is this to be understood ?\n",
      "[[90, 9, 30, 4, 20, 2295, 25]]\n",
      "aleke yesu kae nawɔ ? <end>\n",
      "\n",
      "The word rod is translated from the Hebrew word she vet .\n",
      "[[2, 242, 1049, 9, 3395, 28, 2, 3396, 242, 69, 4083, 1]]\n",
      "wo o nu tso nya i si gɔmee nye biblia me e dzesi le . <end>\n",
      "\n",
      "To the Hebrews she vet meant a stick or a staff such as that used by a shepherd .\n",
      "[[4, 2, 1465, 69, 4083, 1374, 6, 2068, 24, 6, 1466, 73, 16, 8, 213, 26, 6, 5258, 1]]\n",
      "esi paulo o nu tso paulo e agbale si nye hebritɔwo ha me ko la kple alakpa tɔ . <end>\n",
      "\n",
      "In this context the rod of authority suggests loving guidance not harsh brutality . Psalm .\n",
      "[[7, 30, 2559, 2, 1049, 3, 762, 1720, 709, 1588, 21, 4084, 4085, 1, 948, 1]]\n",
      "le lɔlɔ me be woabu fɔe kple tohehe gblɔ be woahe xexe yeye a eke meli na ame . lododowo . <end>\n",
      "\n",
      "She vet is often used symbolically in the Bible representing authority .\n",
      "[[69, 4083, 9, 149, 213, 7364, 7, 2, 74, 5259, 762, 1]]\n",
      "wozaa she vet le kpɔ e kpɔ nyate ea u . <end>\n",
      "\n",
      "When referring to parental authority the rod does not refer exclusively to physical punishment .\n",
      "[[42, 2296, 4, 1051, 762, 2, 1049, 123, 21, 5260, 2902, 4, 260, 1190, 1]]\n",
      "ne dzilawo lea nu o o o o o o si gblea eyi i si me nunya le la u . <end>\n",
      "\n",
      "It encompasses all forms of discipline which most often need not be physical .\n",
      "[[11, 7365, 50, 1122, 3, 383, 101, 97, 149, 157, 21, 20, 260, 1]]\n",
      "elɔ e edzi be ke ame o o o ke menye nusi tututu o lɔlɔ amee o . <end>\n",
      "\n",
      "And when physical discipline is employed it is usually because other methods have proved unsuccessful .\n",
      "[[5, 42, 260, 383, 9, 3397, 11, 9, 332, 98, 77, 1123, 23, 1375, 7366, 1]]\n",
      "eye ne ame a ewo ha kpɔe dze sii be ame bubu ha la ame bubu ha medze agbagba o . <end>\n",
      "\n",
      "Proverbs says that foolishness is tied up anchored NJB deep rooted The New English Bible with the heart of the one receiving physical discipline .\n",
      "[[420, 111, 8, 2896, 9, 1876, 80, 5261, 7367, 1467, 3398, 2, 91, 1192, 74, 17, 2, 337, 3, 2, 34, 2297, 260, 383, 1]]\n",
      "lododowo gblɔ be bometsitsi bla da seke njb xɔ a e the new english bible e amesi ame dzi u ua e amesi nye the new york . <end>\n",
      "\n",
      "More than mere childish frivolity is involved .\n",
      "[[53, 72, 949, 7368, 7369, 9, 560, 1]]\n",
      "menye evi o koe agbe o ke bo nya e vi e koe wona gblɔm wole o . <end>\n",
      "\n",
      "How Should Discipline Be Administered ?\n",
      "[[90, 119, 383, 20, 1721, 25]]\n",
      "aleke woahe ? <end>\n",
      "\n",
      "In the Bible discipline is consistently linked with love and mildness not with anger and brutality .\n",
      "[[7, 2, 74, 383, 9, 5262, 2069, 17, 161, 5, 5263, 21, 17, 643, 5, 4085, 1]]\n",
      "taflatse miyi edzi vea amee eye edzea egɔme eye wozu nuwo hele e i nyuiwo . <end>\n",
      "\n",
      "The skillful counselor should be gentle toward all . . . restrained under evil instructing with mildness those not favorably disposed . Timothy .\n",
      "[[2, 7370, 2298, 119, 20, 1722, 561, 50, 1, 1, 1, 5264, 191, 1589, 7371, 17, 5263, 94, 21, 5265, 4086, 1, 1266, 1]]\n",
      "be woabu nanɔ anyi tsa e kaka edzi le amesi te e la ame a eke si le anyigba dzi be woanɔ anyi le amesiwo kata wum wu la . . . <end>\n",
      "\n",
      "Therefore discipline is not an emotional outlet for the parent .\n",
      "[[543, 383, 9, 21, 37, 448, 4087, 10, 2, 314, 1]]\n",
      "eyata tohehe e mɔ na ame ola a e e ate u a e e nu kura o . <end>\n",
      "\n",
      "Rather it is a method of instruction .\n",
      "[[502, 11, 9, 6, 1050, 3, 2299, 1]]\n",
      "ke bo eyae nɔ dodom . <end>\n",
      "\n",
      "As such it should teach an erring child .\n",
      "[[16, 73, 11, 119, 598, 37, 7372, 78, 1]]\n",
      "le evi a e si gblɔ be ele be woanye yayra tso ame si . <end>\n",
      "\n",
      "When administered in anger physical discipline teaches the wrong lesson .\n",
      "[[42, 1721, 7, 643, 260, 383, 1881, 2, 486, 3399, 1]]\n",
      "ne wotsɔ dziku na amee la nu gbegble a e e e susu a e vɛ . <end>\n",
      "\n",
      "It serves the need of the parent not that of the child .\n",
      "[[11, 2070, 2, 157, 3, 2, 314, 21, 8, 3, 2, 78, 1]]\n",
      "enana nyuie be woadze evi si ava ava . <end>\n",
      "\n",
      "Furthermore effective discipline has boundaries .\n",
      "[[1191, 826, 383, 52, 5266, 1]]\n",
      "hekpe e u la se utɔ a u si li e e u . <end>\n",
      "\n",
      " I shall have to chastise you to the proper degree Jehovah says to his people at Jeremiah .\n",
      "[[12, 1468, 23, 4, 7373, 15, 4, 2, 904, 2903, 45, 111, 4, 35, 64, 33, 2561, 1]]\n",
      "yehowa e fia be woagbe israel viwo be yeaxɔ amesiame si . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, test_sent in enumerate(raw_data_en[:50]):\n",
    "    test_sequence = normalize_eng(test_sent)\n",
    "    predict(test_sequence)\n",
    "    # print(÷)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church service is good\n",
      "[[117, 403, 9, 165]]\n",
      "egbegbe ha wɔ akpa vevi a e <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(1, 8, 4, 4), dtype=float32, numpy=\n",
       "  array([[[[0.26653972, 0.3900016 , 0.16261663, 0.18084213],\n",
       "           [0.23550795, 0.37519023, 0.10318404, 0.28611782],\n",
       "           [0.13542521, 0.28190026, 0.08690803, 0.49576643],\n",
       "           [0.23223537, 0.29360396, 0.10685881, 0.36730188]],\n",
       "  \n",
       "          [[0.2618468 , 0.24410997, 0.2281403 , 0.2659029 ],\n",
       "           [0.4617364 , 0.19400708, 0.17258114, 0.1716754 ],\n",
       "           [0.62679476, 0.14767483, 0.10717851, 0.11835195],\n",
       "           [0.70720446, 0.16180487, 0.05448499, 0.07650569]],\n",
       "  \n",
       "          [[0.16626632, 0.18580084, 0.495854  , 0.15207876],\n",
       "           [0.2222835 , 0.44833434, 0.28615052, 0.04323165],\n",
       "           [0.13975926, 0.25916174, 0.41096324, 0.19011581],\n",
       "           [0.1671756 , 0.2773282 , 0.35323322, 0.20226294]],\n",
       "  \n",
       "          [[0.16046485, 0.3943131 , 0.04002463, 0.40519744],\n",
       "           [0.3293221 , 0.36046845, 0.106892  , 0.2033175 ],\n",
       "           [0.1564522 , 0.3142563 , 0.1826611 , 0.34663036],\n",
       "           [0.22603844, 0.2760603 , 0.21258877, 0.28531244]],\n",
       "  \n",
       "          [[0.08684453, 0.7333957 , 0.040389  , 0.13937078],\n",
       "           [0.17418118, 0.58554494, 0.09663673, 0.1436372 ],\n",
       "           [0.08423548, 0.6962797 , 0.04687318, 0.17261164],\n",
       "           [0.04147454, 0.80618936, 0.02762903, 0.12470713]],\n",
       "  \n",
       "          [[0.02370175, 0.15222143, 0.04735449, 0.7767223 ],\n",
       "           [0.19553627, 0.42251015, 0.21461856, 0.16733506],\n",
       "           [0.23890264, 0.22223842, 0.13979527, 0.3990637 ],\n",
       "           [0.44635397, 0.31616902, 0.10096444, 0.13651252]],\n",
       "  \n",
       "          [[0.02625118, 0.04188236, 0.03498658, 0.89687985],\n",
       "           [0.09846759, 0.080216  , 0.05430643, 0.7670099 ],\n",
       "           [0.17925204, 0.14384942, 0.04028793, 0.63661057],\n",
       "           [0.22818835, 0.51455474, 0.11828217, 0.13897474]],\n",
       "  \n",
       "          [[0.18830921, 0.6092379 , 0.08541202, 0.11704088],\n",
       "           [0.36871836, 0.32900167, 0.16636604, 0.13591403],\n",
       "           [0.45296666, 0.45591018, 0.01695353, 0.07416961],\n",
       "           [0.40062386, 0.42919993, 0.06697129, 0.1032049 ]]]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 4, 4), dtype=float32, numpy=\n",
       "  array([[[[0.5169547 , 0.05058265, 0.0706549 , 0.36180773],\n",
       "           [0.39299425, 0.10882235, 0.20178333, 0.29640004],\n",
       "           [0.37926725, 0.07239256, 0.08000492, 0.46833527],\n",
       "           [0.29318976, 0.1656437 , 0.20271933, 0.33844715]],\n",
       "  \n",
       "          [[0.45225975, 0.15678447, 0.0225526 , 0.36840323],\n",
       "           [0.14556743, 0.1678115 , 0.02347809, 0.6631429 ],\n",
       "           [0.13195315, 0.1147218 , 0.01546086, 0.73786414],\n",
       "           [0.21620686, 0.26353836, 0.02763901, 0.49261576]],\n",
       "  \n",
       "          [[0.27436978, 0.59257203, 0.09013059, 0.04292756],\n",
       "           [0.23179948, 0.6922294 , 0.0371223 , 0.03884881],\n",
       "           [0.31379503, 0.5285152 , 0.05738271, 0.10030706],\n",
       "           [0.2887391 , 0.533386  , 0.04129124, 0.13658364]],\n",
       "  \n",
       "          [[0.72614914, 0.11874878, 0.01704035, 0.13806169],\n",
       "           [0.8024868 , 0.08332435, 0.01926484, 0.09492394],\n",
       "           [0.89458925, 0.03435047, 0.01322722, 0.05783306],\n",
       "           [0.8665058 , 0.0709241 , 0.02042369, 0.0421464 ]],\n",
       "  \n",
       "          [[0.57172424, 0.1336128 , 0.05932148, 0.23534152],\n",
       "           [0.10714424, 0.28883702, 0.17873351, 0.42528525],\n",
       "           [0.11670059, 0.10511637, 0.09150127, 0.68668175],\n",
       "           [0.11641242, 0.08833947, 0.11582513, 0.6794229 ]],\n",
       "  \n",
       "          [[0.74712414, 0.17369993, 0.01852293, 0.06065303],\n",
       "           [0.80029446, 0.12175701, 0.00597799, 0.07197057],\n",
       "           [0.83133894, 0.08800982, 0.00364245, 0.07700885],\n",
       "           [0.83905977, 0.06502272, 0.00921279, 0.08670474]],\n",
       "  \n",
       "          [[0.17046784, 0.20891747, 0.12931821, 0.49129653],\n",
       "           [0.31195998, 0.40335408, 0.07813719, 0.20654872],\n",
       "           [0.08637384, 0.1696174 , 0.11638934, 0.62761945],\n",
       "           [0.1889075 , 0.4896218 , 0.11942264, 0.20204803]],\n",
       "  \n",
       "          [[0.6559653 , 0.2791072 , 0.03512409, 0.02980341],\n",
       "           [0.76419497, 0.15667875, 0.03801109, 0.04111511],\n",
       "           [0.7043037 , 0.18938659, 0.04498618, 0.06132358],\n",
       "           [0.57379204, 0.1267453 , 0.05513074, 0.24433199]]]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 4, 4), dtype=float32, numpy=\n",
       "  array([[[[0.7384012 , 0.13451798, 0.12525609, 0.00182474],\n",
       "           [0.32667983, 0.4836275 , 0.1755783 , 0.01411435],\n",
       "           [0.38509628, 0.27343976, 0.33017516, 0.01128882],\n",
       "           [0.30231264, 0.41852263, 0.23888455, 0.0402802 ]],\n",
       "  \n",
       "          [[0.47639894, 0.09699206, 0.18046694, 0.246142  ],\n",
       "           [0.38452715, 0.12768397, 0.10999402, 0.3777949 ],\n",
       "           [0.30570477, 0.1335823 , 0.12814982, 0.43256316],\n",
       "           [0.10911147, 0.1922745 , 0.26083553, 0.43777847]],\n",
       "  \n",
       "          [[0.3403072 , 0.24311222, 0.30210406, 0.11447655],\n",
       "           [0.21645953, 0.20689195, 0.26092687, 0.31572154],\n",
       "           [0.19940782, 0.13905954, 0.22321302, 0.4383196 ],\n",
       "           [0.24381126, 0.10126671, 0.10086314, 0.5540589 ]],\n",
       "  \n",
       "          [[0.44534168, 0.06502014, 0.29125476, 0.19838345],\n",
       "           [0.31501985, 0.14848264, 0.32437044, 0.21212701],\n",
       "           [0.20976757, 0.05933993, 0.4566329 , 0.2742596 ],\n",
       "           [0.15880181, 0.15411474, 0.3344719 , 0.3526115 ]],\n",
       "  \n",
       "          [[0.27759948, 0.16214444, 0.37213966, 0.1881164 ],\n",
       "           [0.20643973, 0.27993518, 0.29430285, 0.21932225],\n",
       "           [0.26964986, 0.25600928, 0.28557   , 0.18877093],\n",
       "           [0.18204573, 0.24653868, 0.20300801, 0.36840758]],\n",
       "  \n",
       "          [[0.55313575, 0.13982068, 0.18022852, 0.12681505],\n",
       "           [0.40096217, 0.23979425, 0.22290848, 0.13633509],\n",
       "           [0.5310803 , 0.14621034, 0.1927101 , 0.12999922],\n",
       "           [0.44310805, 0.15635888, 0.22746335, 0.1730697 ]],\n",
       "  \n",
       "          [[0.6533379 , 0.12557058, 0.07863872, 0.14245278],\n",
       "           [0.38038474, 0.21771553, 0.09225703, 0.30964267],\n",
       "           [0.66926044, 0.15550995, 0.10910716, 0.06612244],\n",
       "           [0.21031769, 0.28315243, 0.08001193, 0.42651796]],\n",
       "  \n",
       "          [[0.3397749 , 0.24152859, 0.20794232, 0.21075422],\n",
       "           [0.39782584, 0.20087665, 0.20727982, 0.1940177 ],\n",
       "           [0.48018742, 0.17255084, 0.18272044, 0.16454132],\n",
       "           [0.23124312, 0.14613585, 0.25092515, 0.3716959 ]]]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 4, 4), dtype=float32, numpy=\n",
       "  array([[[[0.43441033, 0.43708363, 0.05463162, 0.07387441],\n",
       "           [0.38575345, 0.439429  , 0.06638091, 0.10843669],\n",
       "           [0.25775665, 0.42142695, 0.09262816, 0.22818829],\n",
       "           [0.21382213, 0.46349347, 0.09483043, 0.22785398]],\n",
       "  \n",
       "          [[0.29226765, 0.22337338, 0.24699911, 0.23735987],\n",
       "           [0.2978994 , 0.24158733, 0.2487715 , 0.21174176],\n",
       "           [0.2392897 , 0.32167274, 0.24537575, 0.1936618 ],\n",
       "           [0.14926074, 0.32816863, 0.2684451 , 0.2541255 ]],\n",
       "  \n",
       "          [[0.4379297 , 0.1936693 , 0.14017795, 0.22822306],\n",
       "           [0.40122947, 0.19585036, 0.15136734, 0.25155285],\n",
       "           [0.5001123 , 0.17086823, 0.1422102 , 0.18680926],\n",
       "           [0.39137477, 0.21003754, 0.17995295, 0.21863475]],\n",
       "  \n",
       "          [[0.29481715, 0.43798319, 0.14503406, 0.12216565],\n",
       "           [0.30151376, 0.3585428 , 0.17924501, 0.16069834],\n",
       "           [0.0909121 , 0.45273685, 0.26744378, 0.18890727],\n",
       "           [0.14696097, 0.29748872, 0.29010352, 0.26544675]],\n",
       "  \n",
       "          [[0.3548546 , 0.19782405, 0.09897983, 0.34834152],\n",
       "           [0.23462655, 0.28946713, 0.16343765, 0.3124687 ],\n",
       "           [0.27832776, 0.32121992, 0.22335908, 0.17709327],\n",
       "           [0.26451975, 0.29774266, 0.18248035, 0.2552572 ]],\n",
       "  \n",
       "          [[0.3400885 , 0.2572342 , 0.25189486, 0.15078238],\n",
       "           [0.17320704, 0.21169685, 0.28733167, 0.3277644 ],\n",
       "           [0.14208414, 0.17939432, 0.33330834, 0.3452132 ],\n",
       "           [0.11111994, 0.18607758, 0.21923582, 0.4835666 ]],\n",
       "  \n",
       "          [[0.38004148, 0.213385  , 0.24096392, 0.16560958],\n",
       "           [0.31974167, 0.27184626, 0.19845393, 0.2099582 ],\n",
       "           [0.28622428, 0.23396328, 0.22934467, 0.25046775],\n",
       "           [0.35323602, 0.16056217, 0.18696119, 0.2992406 ]],\n",
       "  \n",
       "          [[0.22927406, 0.34401014, 0.25615767, 0.1705582 ],\n",
       "           [0.17843133, 0.31496334, 0.25872758, 0.24787776],\n",
       "           [0.18878804, 0.3500095 , 0.25815728, 0.20304509],\n",
       "           [0.2341216 , 0.33399707, 0.26453525, 0.16734615]]]],\n",
       "        dtype=float32)>],\n",
       " [<tf.Tensor: shape=(1, 8, 8, 8), dtype=float32, numpy=\n",
       "  array([[[[2.19842598e-01, 5.54694645e-02, 8.29489455e-02,\n",
       "            1.39460519e-01, 5.64125665e-02, 2.77106017e-01,\n",
       "            8.02304745e-02, 8.85294750e-02],\n",
       "           [1.65939569e-01, 8.14458076e-03, 3.96550484e-02,\n",
       "            1.23189867e-01, 6.15853965e-02, 2.93777436e-01,\n",
       "            2.14038789e-01, 9.36693400e-02],\n",
       "           [3.15269865e-02, 1.31867537e-02, 3.93656967e-03,\n",
       "            7.00311810e-02, 3.17530304e-01, 3.10286403e-01,\n",
       "            2.82386858e-02, 2.25263163e-01],\n",
       "           [5.07936001e-01, 3.81148495e-02, 2.39110887e-02,\n",
       "            8.59525241e-03, 4.58095968e-02, 1.37003595e-02,\n",
       "            4.70553599e-02, 3.14877510e-01],\n",
       "           [6.15933180e-01, 1.91092677e-02, 1.39651289e-02,\n",
       "            6.19812049e-02, 1.22398436e-02, 9.74888280e-02,\n",
       "            5.09925038e-02, 1.28290042e-01],\n",
       "           [8.19971621e-01, 7.72525847e-04, 4.13452229e-03,\n",
       "            8.93102400e-03, 2.21947674e-02, 6.78676227e-03,\n",
       "            2.13629920e-02, 1.15845710e-01],\n",
       "           [1.92793459e-01, 5.38900727e-03, 1.87854469e-02,\n",
       "            8.22974965e-02, 2.42736608e-01, 1.86815292e-01,\n",
       "            9.17996839e-02, 1.79383025e-01],\n",
       "           [3.44507918e-02, 6.36746734e-03, 2.71313507e-02,\n",
       "            2.36755237e-01, 2.35244166e-02, 6.08797908e-01,\n",
       "            3.80605273e-02, 2.49123871e-02]],\n",
       "  \n",
       "          [[1.92195654e-01, 1.29705653e-01, 9.21363384e-02,\n",
       "            1.94484428e-01, 7.90123194e-02, 1.21533327e-01,\n",
       "            1.06826760e-01, 8.41055736e-02],\n",
       "           [7.59721756e-01, 3.77541333e-02, 1.26890158e-02,\n",
       "            5.15059046e-02, 6.65575964e-03, 5.53179951e-03,\n",
       "            2.77349781e-02, 9.84066874e-02],\n",
       "           [1.65277898e-01, 4.50179987e-02, 7.33887181e-02,\n",
       "            2.41590947e-01, 5.67071885e-02, 1.93003431e-01,\n",
       "            1.17613234e-01, 1.07400596e-01],\n",
       "           [1.83775425e-01, 5.04292659e-02, 1.05869457e-01,\n",
       "            2.53826350e-01, 6.14478104e-02, 1.19438320e-01,\n",
       "            8.14396292e-02, 1.43773660e-01],\n",
       "           [2.68140227e-01, 1.60141718e-02, 3.96716706e-02,\n",
       "            1.24770448e-01, 3.39280032e-02, 1.45332128e-01,\n",
       "            1.48529723e-01, 2.23613650e-01],\n",
       "           [2.85615146e-01, 2.50012074e-02, 2.01409146e-01,\n",
       "            9.51393917e-02, 3.29346582e-02, 1.73390582e-02,\n",
       "            1.49424359e-01, 1.93137109e-01],\n",
       "           [2.34455436e-01, 3.21351767e-01, 9.24177319e-02,\n",
       "            8.02024826e-02, 2.44069342e-02, 3.36195417e-02,\n",
       "            9.84977409e-02, 1.15048341e-01],\n",
       "           [2.66646355e-01, 4.22721207e-02, 3.45516913e-02,\n",
       "            2.47048706e-01, 5.87960035e-02, 1.61559373e-01,\n",
       "            1.08006388e-01, 8.11193362e-02]],\n",
       "  \n",
       "          [[1.87762901e-01, 7.09678233e-02, 1.04768589e-01,\n",
       "            1.30791321e-01, 1.17136441e-01, 1.92481011e-01,\n",
       "            9.95748341e-02, 9.65170562e-02],\n",
       "           [4.94410008e-01, 5.79463020e-02, 7.20374212e-02,\n",
       "            3.86036336e-02, 5.78491502e-02, 1.51982084e-01,\n",
       "            2.94084121e-02, 9.77629647e-02],\n",
       "           [5.85761070e-02, 8.34347308e-03, 1.38544336e-01,\n",
       "            3.44154775e-01, 1.40162364e-01, 8.29590410e-02,\n",
       "            1.51345313e-01, 7.59146288e-02],\n",
       "           [2.07340941e-01, 5.30201197e-02, 1.48562854e-02,\n",
       "            4.92548048e-02, 1.19744636e-01, 5.36187068e-02,\n",
       "            2.22314730e-01, 2.79849708e-01],\n",
       "           [8.22187141e-02, 9.10940487e-03, 8.52983911e-03,\n",
       "            1.64398663e-02, 7.82224610e-02, 7.10209489e-01,\n",
       "            3.67952138e-02, 5.84751442e-02],\n",
       "           [2.70239234e-01, 1.41431089e-03, 1.71875414e-02,\n",
       "            3.71610783e-02, 3.37287158e-01, 1.03449620e-01,\n",
       "            1.01528548e-01, 1.31732568e-01],\n",
       "           [3.22762951e-02, 1.15574885e-03, 1.03689134e-02,\n",
       "            4.08572666e-02, 4.49044615e-01, 4.22508538e-01,\n",
       "            9.48960520e-03, 3.42990495e-02],\n",
       "           [1.58257559e-01, 2.21419986e-02, 1.96512993e-02,\n",
       "            4.83015813e-02, 7.30059072e-02, 4.90819335e-01,\n",
       "            1.06927678e-01, 8.08945894e-02]],\n",
       "  \n",
       "          [[1.26776904e-01, 3.48127149e-02, 8.10319334e-02,\n",
       "            5.57845645e-02, 1.70082361e-01, 2.23810285e-01,\n",
       "            1.54960826e-01, 1.52740508e-01],\n",
       "           [1.03174485e-01, 1.20084100e-04, 9.91540402e-03,\n",
       "            1.65303471e-03, 1.61808819e-01, 1.85495228e-01,\n",
       "            2.74953663e-01, 2.62879282e-01],\n",
       "           [5.82407527e-02, 1.78402752e-01, 4.31706719e-02,\n",
       "            2.15360969e-01, 9.84284058e-02, 1.39296561e-01,\n",
       "            1.81292281e-01, 8.58076066e-02],\n",
       "           [2.04232290e-01, 3.74628371e-03, 4.37071808e-02,\n",
       "            1.31325964e-02, 6.56000003e-02, 1.43083215e-01,\n",
       "            1.43357217e-01, 3.83141220e-01],\n",
       "           [7.03444004e-01, 1.26655564e-01, 3.28296013e-02,\n",
       "            5.27055971e-02, 2.44694715e-03, 4.05449653e-03,\n",
       "            2.80460417e-02, 4.98177670e-02],\n",
       "           [4.22981083e-01, 1.23391353e-01, 5.83776459e-02,\n",
       "            1.11019000e-01, 1.97598878e-02, 7.03499606e-03,\n",
       "            8.74395147e-02, 1.69996530e-01],\n",
       "           [3.57026041e-01, 7.55322054e-02, 6.09491952e-02,\n",
       "            1.00331232e-01, 4.43753973e-02, 5.94265088e-02,\n",
       "            1.48752317e-01, 1.53607130e-01],\n",
       "           [1.51645750e-01, 2.75413897e-02, 7.70106316e-02,\n",
       "            4.82333079e-02, 1.97979227e-01, 1.37199879e-01,\n",
       "            2.19708443e-01, 1.40681371e-01]],\n",
       "  \n",
       "          [[8.50893781e-02, 1.65013030e-01, 8.99401009e-02,\n",
       "            1.26232013e-01, 1.67441308e-01, 9.31359828e-02,\n",
       "            1.69036210e-01, 1.04112014e-01],\n",
       "           [7.06646144e-02, 1.42785078e-02, 8.27726796e-02,\n",
       "            3.45683873e-01, 2.15395942e-01, 5.75739257e-02,\n",
       "            1.45137221e-01, 6.84931949e-02],\n",
       "           [1.05574705e-01, 7.15157166e-02, 1.22519292e-01,\n",
       "            2.17923954e-01, 1.53149053e-01, 9.10966173e-02,\n",
       "            1.66622609e-01, 7.15980381e-02],\n",
       "           [5.55752873e-01, 4.36005145e-02, 4.93434221e-02,\n",
       "            7.00824708e-02, 2.55383011e-02, 2.76218429e-02,\n",
       "            7.13945627e-02, 1.56666040e-01],\n",
       "           [1.19519152e-01, 4.81093049e-01, 2.60919966e-02,\n",
       "            1.47792682e-01, 2.38051321e-02, 4.93344814e-02,\n",
       "            2.98415069e-02, 1.22522011e-01],\n",
       "           [1.72957867e-01, 4.82988715e-01, 1.91724468e-02,\n",
       "            9.07633454e-02, 2.13624593e-02, 2.35569049e-02,\n",
       "            3.94530818e-02, 1.49745151e-01],\n",
       "           [9.41440389e-02, 3.24584804e-02, 1.09538227e-01,\n",
       "            1.48921236e-01, 2.24631548e-01, 9.06259045e-02,\n",
       "            1.79048061e-01, 1.20632440e-01],\n",
       "           [2.12105691e-01, 2.92423159e-01, 2.92955879e-02,\n",
       "            1.49972871e-01, 3.73645127e-02, 3.66418697e-02,\n",
       "            1.27423137e-01, 1.14773236e-01]],\n",
       "  \n",
       "          [[3.47959518e-01, 5.79292737e-02, 1.02108575e-01,\n",
       "            1.52402908e-01, 5.05495705e-02, 4.88714576e-02,\n",
       "            1.40104875e-01, 1.00073874e-01],\n",
       "           [7.50375211e-01, 5.89645049e-03, 2.08687559e-02,\n",
       "            2.55442457e-03, 3.35062779e-02, 8.57683830e-03,\n",
       "            6.58740401e-02, 1.12348020e-01],\n",
       "           [2.92928636e-01, 2.14944519e-02, 2.82908622e-02,\n",
       "            1.34519711e-01, 5.58252670e-02, 1.06796168e-01,\n",
       "            2.17794850e-01, 1.42350078e-01],\n",
       "           [3.55017513e-01, 8.78142864e-02, 1.06072702e-01,\n",
       "            2.96952017e-02, 3.19886655e-01, 2.84643620e-02,\n",
       "            1.02838874e-02, 6.27653375e-02],\n",
       "           [4.06856209e-01, 5.40453829e-02, 3.74494605e-02,\n",
       "            4.54429016e-02, 2.48736925e-02, 1.24344483e-01,\n",
       "            1.84141308e-01, 1.22846536e-01],\n",
       "           [5.10815859e-01, 8.31451267e-03, 2.28362493e-02,\n",
       "            1.25801666e-02, 2.24153519e-01, 8.87922645e-02,\n",
       "            4.30159122e-02, 8.94915015e-02],\n",
       "           [1.50786832e-01, 1.46209337e-02, 1.03254924e-02,\n",
       "            4.45236154e-02, 7.34385327e-02, 4.51667249e-01,\n",
       "            1.43906310e-01, 1.10731028e-01],\n",
       "           [8.02858025e-02, 3.48760677e-03, 1.34849604e-02,\n",
       "            1.64613929e-02, 6.34673191e-03, 5.03242798e-02,\n",
       "            7.90778637e-01, 3.88305932e-02]],\n",
       "  \n",
       "          [[1.41985208e-01, 8.27270001e-02, 1.30116388e-01,\n",
       "            1.07305318e-01, 1.75820813e-01, 1.65734008e-01,\n",
       "            1.02423906e-01, 9.38872918e-02],\n",
       "           [1.32167172e-02, 4.32737097e-02, 1.09229162e-02,\n",
       "            9.95713770e-02, 5.11469185e-01, 3.02687347e-01,\n",
       "            1.32023441e-02, 5.65635320e-03],\n",
       "           [2.68196054e-02, 7.68895727e-03, 1.94055997e-02,\n",
       "            8.93161222e-02, 6.58638954e-01, 1.41153350e-01,\n",
       "            4.38376851e-02, 1.31397489e-02],\n",
       "           [1.11831285e-01, 1.87378675e-02, 3.08814719e-02,\n",
       "            3.45977321e-02, 2.89080530e-01, 2.54530072e-01,\n",
       "            1.78381726e-01, 8.19592997e-02],\n",
       "           [3.07525963e-01, 2.94869225e-02, 5.39110899e-02,\n",
       "            2.94899102e-02, 6.77255988e-02, 1.71201259e-01,\n",
       "            1.52812153e-01, 1.87847093e-01],\n",
       "           [1.32986829e-01, 5.06268181e-02, 8.71156752e-02,\n",
       "            1.15749635e-01, 9.86294970e-02, 2.78549373e-01,\n",
       "            1.20194159e-01, 1.16147965e-01],\n",
       "           [1.17280312e-01, 1.59843326e-01, 6.09911568e-02,\n",
       "            1.01979360e-01, 1.31864086e-01, 2.48345777e-01,\n",
       "            9.38062295e-02, 8.58896896e-02],\n",
       "           [7.87990615e-02, 1.01134308e-01, 1.20155893e-01,\n",
       "            1.02564238e-01, 2.94819564e-01, 2.10933924e-01,\n",
       "            4.26363274e-02, 4.89567071e-02]],\n",
       "  \n",
       "          [[1.75552949e-01, 1.21648222e-01, 8.71236846e-02,\n",
       "            1.01298898e-01, 1.11695535e-01, 1.31988704e-01,\n",
       "            1.75327063e-01, 9.53650028e-02],\n",
       "           [6.10863268e-01, 3.63468565e-03, 7.53867393e-03,\n",
       "            1.50034860e-01, 9.37530492e-03, 6.90520089e-03,\n",
       "            5.27089685e-02, 1.58939064e-01],\n",
       "           [3.70298386e-01, 3.19792591e-02, 2.53943130e-02,\n",
       "            1.43322915e-01, 9.53480825e-02, 1.28721699e-01,\n",
       "            9.89907905e-02, 1.05944544e-01],\n",
       "           [1.18596755e-01, 3.45487654e-01, 1.33966297e-01,\n",
       "            1.19566783e-01, 4.48591299e-02, 3.15271951e-02,\n",
       "            7.21092597e-02, 1.33886918e-01],\n",
       "           [2.49392062e-01, 3.25776800e-03, 1.56897642e-02,\n",
       "            1.98716819e-01, 3.58917154e-02, 6.08888641e-02,\n",
       "            6.69082776e-02, 3.69254738e-01],\n",
       "           [4.00822014e-02, 2.30598846e-03, 8.27923510e-03,\n",
       "            6.85762942e-01, 8.75308216e-02, 6.03251271e-02,\n",
       "            5.04120886e-02, 6.53016567e-02],\n",
       "           [1.01057589e-02, 1.12255955e-04, 2.16662209e-03,\n",
       "            3.05283070e-01, 1.19088583e-01, 5.56488514e-01,\n",
       "            2.02210224e-03, 4.73308051e-03],\n",
       "           [2.49443218e-01, 1.66647479e-01, 8.75044316e-02,\n",
       "            7.80637711e-02, 4.81051616e-02, 3.29850502e-02,\n",
       "            1.46971524e-01, 1.90279439e-01]]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 8), dtype=float32, numpy=\n",
       "  array([[[[3.04367449e-02, 4.13314104e-02, 1.95559725e-01,\n",
       "            2.37075627e-01, 1.25461325e-01, 2.26880297e-01,\n",
       "            5.90745732e-02, 8.41802508e-02],\n",
       "           [2.29728699e-01, 2.39499137e-02, 4.11628745e-02,\n",
       "            1.85189381e-01, 2.29541332e-01, 1.83088735e-01,\n",
       "            5.17235920e-02, 5.56153879e-02],\n",
       "           [8.07046518e-02, 7.70587800e-03, 2.03368347e-02,\n",
       "            1.49751455e-01, 3.82895291e-01, 2.71436870e-01,\n",
       "            7.29410946e-02, 1.42278252e-02],\n",
       "           [3.80375713e-01, 1.54964432e-01, 3.75943407e-02,\n",
       "            4.30615172e-02, 1.65336341e-01, 1.08651824e-01,\n",
       "            3.34049910e-02, 7.66108111e-02],\n",
       "           [1.54158130e-01, 7.33423010e-02, 1.35320932e-01,\n",
       "            6.70637116e-02, 1.24318071e-01, 2.35349670e-01,\n",
       "            3.98921855e-02, 1.70554966e-01],\n",
       "           [5.00517547e-01, 1.62410870e-01, 3.16684246e-02,\n",
       "            2.46548988e-02, 4.54911590e-02, 1.12240344e-01,\n",
       "            2.24078307e-03, 1.20775893e-01],\n",
       "           [5.00376672e-02, 4.70013112e-01, 1.66325737e-02,\n",
       "            1.60028681e-01, 6.37626229e-03, 1.94969803e-01,\n",
       "            3.63907847e-03, 9.83027518e-02],\n",
       "           [8.49724934e-03, 1.43636158e-03, 5.94016612e-02,\n",
       "            5.12223653e-02, 1.89266801e-01, 5.15828073e-01,\n",
       "            1.54735848e-01, 1.96116250e-02]],\n",
       "  \n",
       "          [[2.81111211e-01, 1.29381984e-01, 8.86516646e-02,\n",
       "            1.39494151e-01, 1.43338963e-01, 7.31060505e-02,\n",
       "            4.81204204e-02, 9.67955813e-02],\n",
       "           [1.63382679e-01, 4.21928354e-02, 1.46713242e-01,\n",
       "            1.42350793e-01, 1.95173010e-01, 1.35812953e-01,\n",
       "            8.98743421e-02, 8.45001340e-02],\n",
       "           [2.89064288e-01, 8.78036916e-02, 1.41842831e-02,\n",
       "            4.25291479e-01, 9.74671170e-03, 8.85595828e-02,\n",
       "            1.92517024e-02, 6.60982877e-02],\n",
       "           [4.22193527e-01, 5.24157472e-02, 8.84984359e-02,\n",
       "            1.93597563e-02, 6.65560886e-02, 6.75685629e-02,\n",
       "            2.53078610e-01, 3.03292461e-02],\n",
       "           [1.74582452e-01, 9.22661647e-02, 5.27886264e-02,\n",
       "            3.28673989e-01, 1.39058903e-01, 9.50622484e-02,\n",
       "            4.14971337e-02, 7.60705173e-02],\n",
       "           [1.38531864e-01, 1.93169601e-02, 7.77785778e-02,\n",
       "            7.68769234e-02, 4.40821439e-01, 7.00393990e-02,\n",
       "            1.46796316e-01, 2.98384931e-02],\n",
       "           [4.55507070e-01, 6.00206330e-02, 8.54204223e-02,\n",
       "            1.08193882e-01, 1.28154978e-01, 5.60235977e-02,\n",
       "            6.88910857e-02, 3.77884023e-02],\n",
       "           [2.56433785e-01, 1.73999041e-01, 2.33008359e-02,\n",
       "            3.15912664e-01, 1.79701019e-02, 1.09858409e-01,\n",
       "            1.11309588e-02, 9.13941264e-02]],\n",
       "  \n",
       "          [[1.05956092e-01, 1.35835841e-01, 6.72569051e-02,\n",
       "            1.66247174e-01, 1.10848121e-01, 1.57571957e-01,\n",
       "            1.13464184e-01, 1.42819628e-01],\n",
       "           [1.05156295e-01, 1.48194656e-01, 1.15625925e-01,\n",
       "            2.50632346e-01, 9.33594480e-02, 1.55671686e-01,\n",
       "            2.23753005e-02, 1.08984403e-01],\n",
       "           [5.09521604e-01, 5.65788858e-02, 1.74330007e-02,\n",
       "            1.86946645e-01, 9.57591757e-02, 2.46279296e-02,\n",
       "            2.67374068e-02, 8.23953226e-02],\n",
       "           [4.38239276e-01, 5.87877929e-02, 8.17080811e-02,\n",
       "            4.62136120e-02, 2.11145058e-01, 1.21531561e-01,\n",
       "            2.91976724e-02, 1.31768640e-02],\n",
       "           [6.38380110e-01, 4.91730757e-02, 8.02834630e-02,\n",
       "            2.57017538e-02, 3.35672945e-02, 4.99999821e-02,\n",
       "            8.86010751e-02, 3.42932940e-02],\n",
       "           [4.08446550e-01, 6.52308315e-02, 1.06381953e-01,\n",
       "            6.85273334e-02, 1.03457227e-01, 1.00510664e-01,\n",
       "            9.46830213e-02, 5.27624264e-02],\n",
       "           [2.16202110e-01, 5.60259335e-02, 7.73641765e-02,\n",
       "            1.29852518e-01, 1.73039705e-01, 1.47071794e-01,\n",
       "            1.09582685e-01, 9.08610597e-02],\n",
       "           [3.50324929e-01, 5.71070723e-02, 2.71020364e-02,\n",
       "            1.32309556e-01, 7.08757937e-02, 7.62010366e-02,\n",
       "            1.76927283e-01, 1.09152310e-01]],\n",
       "  \n",
       "          [[1.89907461e-01, 1.53245136e-01, 6.99070916e-02,\n",
       "            6.11951798e-02, 2.39388779e-01, 1.08484112e-01,\n",
       "            4.94760349e-02, 1.28396213e-01],\n",
       "           [4.06055391e-01, 5.19945100e-02, 3.36122960e-02,\n",
       "            7.12791830e-02, 1.95632055e-01, 1.14940062e-01,\n",
       "            6.73571900e-02, 5.91293685e-02],\n",
       "           [1.33793816e-01, 7.23005086e-02, 4.30486612e-02,\n",
       "            2.00916484e-01, 1.50380731e-01, 2.56927162e-01,\n",
       "            1.17502049e-01, 2.51305662e-02],\n",
       "           [1.79402083e-01, 3.10549915e-01, 9.78577062e-02,\n",
       "            9.89416316e-02, 9.35384333e-02, 7.25043342e-02,\n",
       "            6.19681738e-02, 8.52376893e-02],\n",
       "           [2.17587471e-01, 4.85764556e-02, 4.51676138e-02,\n",
       "            1.90699130e-01, 2.54439302e-02, 5.44907935e-02,\n",
       "            3.80125523e-01, 3.79090868e-02],\n",
       "           [3.19137275e-01, 2.08694130e-01, 6.43856451e-02,\n",
       "            9.19767469e-02, 1.02692246e-01, 6.30314946e-02,\n",
       "            8.72296765e-02, 6.28528222e-02],\n",
       "           [4.26201969e-01, 1.25184268e-01, 4.67891805e-02,\n",
       "            6.26025870e-02, 4.70709130e-02, 5.41407168e-02,\n",
       "            7.66804963e-02, 1.61329865e-01],\n",
       "           [3.11903894e-01, 6.52756095e-02, 4.47228216e-02,\n",
       "            5.20644449e-02, 1.68031469e-01, 2.24348933e-01,\n",
       "            8.67679715e-02, 4.68848050e-02]],\n",
       "  \n",
       "          [[7.29492530e-02, 8.53080377e-02, 1.35432243e-01,\n",
       "            1.13113075e-01, 2.46434197e-01, 1.40141636e-01,\n",
       "            1.18343145e-01, 8.82784724e-02],\n",
       "           [1.49559617e-01, 9.93119031e-02, 5.05188294e-02,\n",
       "            7.96213523e-02, 1.96480423e-01, 3.42603683e-01,\n",
       "            4.77123037e-02, 3.41918394e-02],\n",
       "           [1.34073392e-01, 1.90594420e-01, 7.75843486e-02,\n",
       "            7.22429901e-02, 2.03797564e-01, 2.13915020e-01,\n",
       "            7.12660402e-02, 3.65262255e-02],\n",
       "           [9.74591374e-02, 2.04586670e-01, 1.16836227e-01,\n",
       "            9.54287350e-02, 1.25958428e-01, 2.55952030e-01,\n",
       "            5.97178265e-02, 4.40609939e-02],\n",
       "           [3.73389006e-01, 2.94084549e-01, 3.92666534e-02,\n",
       "            4.16192226e-02, 2.93729454e-02, 9.59621519e-02,\n",
       "            5.12736440e-02, 7.50318244e-02],\n",
       "           [3.52091998e-01, 3.88009906e-01, 1.18390257e-02,\n",
       "            1.80941168e-02, 1.47363683e-02, 5.13944551e-02,\n",
       "            7.02212974e-02, 9.36129093e-02],\n",
       "           [1.71064034e-01, 1.98415861e-01, 5.96818179e-02,\n",
       "            7.38506243e-02, 4.38890792e-02, 9.37674716e-02,\n",
       "            1.21942841e-01, 2.37388313e-01],\n",
       "           [1.70204677e-02, 5.81768006e-02, 8.58996436e-02,\n",
       "            1.33791849e-01, 3.34801644e-01, 3.19503307e-01,\n",
       "            3.12591307e-02, 1.95471514e-02]],\n",
       "  \n",
       "          [[3.07019293e-01, 1.25164688e-01, 5.59542291e-02,\n",
       "            1.35029495e-01, 7.50161707e-02, 2.01084331e-01,\n",
       "            5.21898158e-02, 4.85420004e-02],\n",
       "           [3.84009421e-01, 4.13560793e-02, 6.54573217e-02,\n",
       "            8.47334862e-02, 1.14926703e-01, 1.77007705e-01,\n",
       "            9.82056931e-02, 3.43036018e-02],\n",
       "           [2.43290052e-01, 8.74599889e-02, 7.62273520e-02,\n",
       "            1.12360492e-01, 6.14200346e-02, 1.79587632e-01,\n",
       "            4.71490063e-02, 1.92505389e-01],\n",
       "           [2.60473222e-01, 2.13212948e-02, 3.64496708e-02,\n",
       "            5.40826507e-02, 3.23496580e-01, 2.20447332e-01,\n",
       "            5.97067140e-02, 2.40225364e-02],\n",
       "           [2.94454873e-01, 8.27748030e-02, 8.86598602e-02,\n",
       "            1.00229986e-01, 9.27606225e-02, 1.32435367e-01,\n",
       "            1.32892877e-01, 7.57916942e-02],\n",
       "           [2.80100167e-01, 5.39686866e-02, 6.81014135e-02,\n",
       "            9.42080915e-02, 2.20024094e-01, 1.18612513e-01,\n",
       "            8.38967487e-02, 8.10883492e-02],\n",
       "           [3.58151197e-02, 4.59318683e-02, 4.93486449e-02,\n",
       "            9.27059129e-02, 2.29491413e-01, 4.24064577e-01,\n",
       "            6.60215840e-02, 5.66209927e-02],\n",
       "           [2.41220504e-01, 9.76783410e-02, 6.49921149e-02,\n",
       "            1.07546926e-01, 7.61419982e-02, 3.02334756e-01,\n",
       "            7.91879445e-02, 3.08974013e-02]],\n",
       "  \n",
       "          [[2.01796621e-01, 3.49569097e-02, 9.60790440e-02,\n",
       "            8.33994001e-02, 1.87434480e-01, 1.59370869e-01,\n",
       "            1.54160634e-01, 8.28020275e-02],\n",
       "           [2.95809537e-01, 7.43255988e-02, 9.63905603e-02,\n",
       "            1.39253825e-01, 1.78413823e-01, 9.91219655e-02,\n",
       "            3.94914038e-02, 7.71933347e-02],\n",
       "           [1.68197080e-02, 1.17082885e-02, 6.03675982e-03,\n",
       "            3.36980343e-01, 4.92612831e-02, 5.67048669e-01,\n",
       "            8.45514995e-04, 1.12993661e-02],\n",
       "           [1.62100717e-01, 1.50657371e-01, 6.16061240e-02,\n",
       "            5.88851161e-02, 2.04684108e-01, 1.46429628e-01,\n",
       "            1.64154202e-01, 5.14827296e-02],\n",
       "           [1.95468571e-02, 2.66998746e-02, 2.75599137e-02,\n",
       "            3.69662404e-01, 7.94113949e-02, 4.41055357e-01,\n",
       "            4.43244865e-03, 3.16317566e-02],\n",
       "           [4.87785935e-02, 4.50372770e-02, 1.21482790e-01,\n",
       "            1.49024531e-01, 3.03003699e-01, 2.30856478e-01,\n",
       "            7.04447925e-02, 3.13717835e-02],\n",
       "           [1.15517840e-01, 3.03030480e-02, 8.34671184e-02,\n",
       "            8.81433040e-02, 3.79255861e-01, 1.44483492e-01,\n",
       "            9.09015834e-02, 6.79277480e-02],\n",
       "           [1.28879547e-01, 4.81737778e-02, 3.63090746e-02,\n",
       "            4.57913339e-01, 1.26985073e-01, 1.52945474e-01,\n",
       "            1.28127320e-03, 4.75123934e-02]],\n",
       "  \n",
       "          [[5.07009812e-02, 2.72152703e-02, 5.77230453e-02,\n",
       "            1.21640138e-01, 2.99455553e-01, 3.04083139e-01,\n",
       "            1.20375663e-01, 1.88061930e-02],\n",
       "           [2.51923800e-02, 2.84341853e-02, 1.16154350e-01,\n",
       "            6.30432665e-02, 2.68528998e-01, 4.76306438e-01,\n",
       "            1.57674085e-02, 6.57298462e-03],\n",
       "           [5.51048703e-02, 1.50602283e-02, 9.73376911e-03,\n",
       "            2.34994277e-01, 2.42069080e-01, 3.20597887e-01,\n",
       "            1.20347895e-01, 2.09197472e-03],\n",
       "           [1.85749769e-01, 1.94249097e-02, 1.49757326e-01,\n",
       "            5.70807569e-02, 2.42219895e-01, 2.35286772e-01,\n",
       "            3.14689167e-02, 7.90116414e-02],\n",
       "           [3.34396780e-01, 1.55373672e-02, 4.75409739e-02,\n",
       "            1.16877109e-01, 2.42247909e-01, 8.89294297e-02,\n",
       "            1.28678083e-01, 2.57923491e-02],\n",
       "           [1.98133335e-01, 2.29773354e-02, 7.87489265e-02,\n",
       "            1.18569076e-01, 1.86481774e-01, 2.10454911e-01,\n",
       "            1.38767481e-01, 4.58672009e-02],\n",
       "           [6.17275722e-02, 4.55334224e-03, 4.96592978e-03,\n",
       "            1.32195622e-01, 6.59586638e-02, 6.74541891e-01,\n",
       "            4.48188558e-02, 1.12380758e-02],\n",
       "           [1.56401675e-02, 4.74180840e-03, 6.32912852e-03,\n",
       "            8.06762204e-02, 6.86966702e-02, 7.90622175e-01,\n",
       "            3.28310244e-02, 4.62764379e-04]]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 8), dtype=float32, numpy=\n",
       "  array([[[[0.13900512, 0.19752659, 0.06139206, 0.05483214, 0.11697222,\n",
       "            0.2234199 , 0.1099825 , 0.09686943],\n",
       "           [0.1521795 , 0.18935747, 0.02485838, 0.31130785, 0.06808913,\n",
       "            0.11951417, 0.06846033, 0.0662332 ],\n",
       "           [0.00782829, 0.29112613, 0.00158152, 0.37324136, 0.0317139 ,\n",
       "            0.27380562, 0.00525331, 0.01544988],\n",
       "           [0.12077203, 0.24171679, 0.02499817, 0.04864421, 0.02849202,\n",
       "            0.33455947, 0.14572357, 0.05509367],\n",
       "           [0.0686832 , 0.2284041 , 0.00780448, 0.25000694, 0.07762484,\n",
       "            0.2953426 , 0.0222319 , 0.04990191],\n",
       "           [0.10088761, 0.15686235, 0.01488698, 0.31275645, 0.0385881 ,\n",
       "            0.265076  , 0.0306552 , 0.08028739],\n",
       "           [0.11155256, 0.12324049, 0.12814955, 0.10629094, 0.11734499,\n",
       "            0.16573085, 0.07225672, 0.17543387],\n",
       "           [0.02055742, 0.22222644, 0.00841076, 0.42301628, 0.06729429,\n",
       "            0.21379036, 0.0164442 , 0.02826027]],\n",
       "  \n",
       "          [[0.15186955, 0.04381345, 0.06880353, 0.25354993, 0.13460122,\n",
       "            0.21599773, 0.04895366, 0.08241098],\n",
       "           [0.1055645 , 0.03735743, 0.09776201, 0.24738964, 0.1317399 ,\n",
       "            0.20606588, 0.07065883, 0.10346176],\n",
       "           [0.22279239, 0.5744802 , 0.0604535 , 0.03646697, 0.01036553,\n",
       "            0.02687231, 0.04532344, 0.02324564],\n",
       "           [0.4392693 , 0.14355446, 0.04868627, 0.07292099, 0.03529391,\n",
       "            0.07648659, 0.13615207, 0.04763647],\n",
       "           [0.19250652, 0.15258887, 0.14649603, 0.133048  , 0.07231099,\n",
       "            0.054365  , 0.15327998, 0.09540459],\n",
       "           [0.4867215 , 0.07002243, 0.0339235 , 0.03279557, 0.02705697,\n",
       "            0.04448175, 0.25998425, 0.045014  ],\n",
       "           [0.19234385, 0.06349797, 0.14434153, 0.08368305, 0.06899133,\n",
       "            0.2211987 , 0.14405322, 0.08189041],\n",
       "           [0.18519941, 0.11374206, 0.08323768, 0.277477  , 0.07680164,\n",
       "            0.07518636, 0.05480368, 0.13355216]],\n",
       "  \n",
       "          [[0.1693188 , 0.12086394, 0.0968447 , 0.10363397, 0.07338762,\n",
       "            0.07022343, 0.13417968, 0.23154795],\n",
       "           [0.16292755, 0.03605683, 0.10098761, 0.21272846, 0.15206079,\n",
       "            0.13120912, 0.0554651 , 0.14856446],\n",
       "           [0.23781422, 0.02155528, 0.05540774, 0.15340611, 0.12955609,\n",
       "            0.06398014, 0.24880433, 0.08947609],\n",
       "           [0.41231647, 0.10816956, 0.08127631, 0.01237247, 0.00858567,\n",
       "            0.03889638, 0.19891776, 0.13946538],\n",
       "           [0.31217256, 0.07695539, 0.04716396, 0.08833383, 0.02880154,\n",
       "            0.04504995, 0.32290095, 0.07862182],\n",
       "           [0.29929128, 0.2139596 , 0.03547131, 0.02166821, 0.00288476,\n",
       "            0.02608989, 0.3448269 , 0.05580804],\n",
       "           [0.20957914, 0.12663093, 0.09892045, 0.24515967, 0.04554518,\n",
       "            0.08809327, 0.06129702, 0.12477428],\n",
       "           [0.32844472, 0.07983725, 0.05336861, 0.09639066, 0.02162754,\n",
       "            0.03588087, 0.20021017, 0.18424025]],\n",
       "  \n",
       "          [[0.15885648, 0.25833318, 0.10593905, 0.07222878, 0.10999837,\n",
       "            0.15957095, 0.02433826, 0.110735  ],\n",
       "           [0.05181346, 0.15161142, 0.39859933, 0.00600257, 0.2490712 ,\n",
       "            0.07048143, 0.00687598, 0.06554464],\n",
       "           [0.29329595, 0.19476117, 0.02416906, 0.17416276, 0.10630156,\n",
       "            0.08922156, 0.03704163, 0.08104627],\n",
       "           [0.30694377, 0.06523037, 0.20184317, 0.01359659, 0.16587348,\n",
       "            0.03028631, 0.01856504, 0.19766128],\n",
       "           [0.63935626, 0.0708037 , 0.03849645, 0.01423859, 0.12146804,\n",
       "            0.04805928, 0.01474648, 0.0528312 ],\n",
       "           [0.72317696, 0.02744869, 0.03741298, 0.02091272, 0.07234944,\n",
       "            0.02948043, 0.01634546, 0.0728733 ],\n",
       "           [0.12365679, 0.13561626, 0.12639715, 0.07174028, 0.20478806,\n",
       "            0.1319136 , 0.11515357, 0.09073426],\n",
       "           [0.15810403, 0.16710044, 0.05847138, 0.26899692, 0.127235  ,\n",
       "            0.13870673, 0.02848428, 0.0529012 ]],\n",
       "  \n",
       "          [[0.05244249, 0.07185207, 0.11317307, 0.03931622, 0.23436911,\n",
       "            0.17583391, 0.22236092, 0.09065213],\n",
       "           [0.10120168, 0.06638815, 0.144846  , 0.0468675 , 0.2056435 ,\n",
       "            0.17678046, 0.10673231, 0.15154041],\n",
       "           [0.14770699, 0.38472992, 0.1070376 , 0.01791459, 0.08542936,\n",
       "            0.1584815 , 0.03677238, 0.06192769],\n",
       "           [0.12411369, 0.0547636 , 0.0993624 , 0.03033884, 0.20070887,\n",
       "            0.11338096, 0.19938178, 0.17794988],\n",
       "           [0.42159906, 0.07538007, 0.07056562, 0.05691298, 0.07758233,\n",
       "            0.06558081, 0.07184415, 0.16053496],\n",
       "           [0.39513865, 0.06401533, 0.03095028, 0.04763469, 0.04044647,\n",
       "            0.03285693, 0.27842775, 0.11052988],\n",
       "           [0.09216776, 0.03809391, 0.10829215, 0.12953526, 0.15524052,\n",
       "            0.11617427, 0.21953422, 0.14096197],\n",
       "           [0.03182196, 0.03370441, 0.20060678, 0.04433867, 0.27670902,\n",
       "            0.30975425, 0.02192694, 0.08113792]],\n",
       "  \n",
       "          [[0.22411099, 0.090858  , 0.13397   , 0.03038636, 0.11793546,\n",
       "            0.06591178, 0.2088066 , 0.12802076],\n",
       "           [0.6292565 , 0.03585233, 0.03908778, 0.0542722 , 0.04016605,\n",
       "            0.08838091, 0.02438073, 0.0886035 ],\n",
       "           [0.1612602 , 0.10104918, 0.0241818 , 0.03548097, 0.04114241,\n",
       "            0.09860421, 0.14495464, 0.39332664],\n",
       "           [0.1446217 , 0.32928345, 0.10597458, 0.01708425, 0.0684373 ,\n",
       "            0.08972785, 0.04212578, 0.20274507],\n",
       "           [0.13114423, 0.11693869, 0.02934673, 0.04429654, 0.01974065,\n",
       "            0.06549697, 0.12882413, 0.464212  ],\n",
       "           [0.04566341, 0.44367802, 0.12475919, 0.07062563, 0.06382458,\n",
       "            0.09451099, 0.05468799, 0.10225023],\n",
       "           [0.421752  , 0.02822659, 0.05321773, 0.05316246, 0.03888331,\n",
       "            0.07239587, 0.20059343, 0.13176857],\n",
       "           [0.32108724, 0.23430443, 0.04511068, 0.01442629, 0.02037884,\n",
       "            0.07071014, 0.07696038, 0.21702203]],\n",
       "  \n",
       "          [[0.07895789, 0.12559925, 0.1515416 , 0.19052516, 0.20871097,\n",
       "            0.08667403, 0.02908399, 0.12890716],\n",
       "           [0.13532573, 0.3800708 , 0.02926366, 0.15865307, 0.01097061,\n",
       "            0.04367152, 0.19466646, 0.04737816],\n",
       "           [0.20082007, 0.21629527, 0.02845781, 0.22433963, 0.0062527 ,\n",
       "            0.01304269, 0.28146476, 0.02932704],\n",
       "           [0.37840232, 0.06417631, 0.0361192 , 0.36013883, 0.02689949,\n",
       "            0.01550401, 0.07865135, 0.04010852],\n",
       "           [0.08274572, 0.2994423 , 0.02624758, 0.20956327, 0.00632042,\n",
       "            0.03011679, 0.31809804, 0.02746588],\n",
       "           [0.24492332, 0.14850931, 0.0590985 , 0.27610165, 0.03132855,\n",
       "            0.02469907, 0.15887554, 0.05646409],\n",
       "           [0.00519645, 0.02415973, 0.06401242, 0.05011507, 0.49329925,\n",
       "            0.27545637, 0.00182616, 0.08593457],\n",
       "           [0.06598577, 0.16372617, 0.01522534, 0.07714167, 0.00223997,\n",
       "            0.02199765, 0.63734233, 0.01634114]],\n",
       "  \n",
       "          [[0.1824357 , 0.14461139, 0.04248664, 0.1279344 , 0.08640556,\n",
       "            0.18030024, 0.13944824, 0.09637788],\n",
       "           [0.09806781, 0.03706211, 0.06370818, 0.09658421, 0.06034131,\n",
       "            0.5565578 , 0.03468076, 0.05299788],\n",
       "           [0.62331885, 0.07798427, 0.06522112, 0.07004961, 0.05082567,\n",
       "            0.04564066, 0.02374281, 0.04321706],\n",
       "           [0.27314442, 0.10447624, 0.02393855, 0.11410689, 0.03868232,\n",
       "            0.3553478 , 0.01931228, 0.07099146],\n",
       "           [0.28153834, 0.08337477, 0.29104158, 0.07418439, 0.03720538,\n",
       "            0.18639092, 0.02376418, 0.0225005 ],\n",
       "           [0.19351344, 0.19604833, 0.21498597, 0.0662529 , 0.06082487,\n",
       "            0.14898564, 0.02248776, 0.09690107],\n",
       "           [0.37565836, 0.10073243, 0.05780163, 0.17438602, 0.04261398,\n",
       "            0.07964151, 0.13029525, 0.0388708 ],\n",
       "           [0.19716418, 0.183093  , 0.07643009, 0.04089578, 0.13833068,\n",
       "            0.07843599, 0.05031253, 0.23533773]]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 8), dtype=float32, numpy=\n",
       "  array([[[[2.17695594e-01, 5.96258305e-02, 1.58353105e-01,\n",
       "            7.43882954e-02, 1.45679966e-01, 1.25965059e-01,\n",
       "            4.53384481e-02, 1.72953665e-01],\n",
       "           [2.11169496e-02, 4.21231687e-02, 3.61689091e-01,\n",
       "            1.53495837e-02, 4.22126688e-02, 6.77723736e-02,\n",
       "            4.80375864e-04, 4.49255764e-01],\n",
       "           [2.00206473e-01, 3.39256935e-02, 9.42387432e-02,\n",
       "            4.48687747e-02, 2.18547404e-01, 2.45476395e-01,\n",
       "            3.76554355e-02, 1.25081107e-01],\n",
       "           [2.14326218e-01, 1.56386986e-01, 1.08391345e-01,\n",
       "            1.56847611e-01, 7.80864358e-02, 7.55161867e-02,\n",
       "            4.58292738e-02, 1.64616004e-01],\n",
       "           [1.78655863e-01, 1.14370562e-01, 1.64893791e-01,\n",
       "            2.47740582e-01, 6.26122802e-02, 9.35966596e-02,\n",
       "            3.68259884e-02, 1.01304308e-01],\n",
       "           [3.22250277e-01, 3.26441415e-02, 1.11527681e-01,\n",
       "            1.46978006e-01, 9.49775726e-02, 8.99528638e-02,\n",
       "            1.50752187e-01, 5.09171933e-02],\n",
       "           [5.23996651e-02, 1.28387868e-01, 1.79788813e-01,\n",
       "            7.31154904e-02, 1.06225632e-01, 2.27474943e-01,\n",
       "            4.05898225e-03, 2.28548586e-01],\n",
       "           [1.29654869e-01, 1.79041009e-02, 1.74584821e-01,\n",
       "            3.71805802e-02, 3.04639995e-01, 1.27988487e-01,\n",
       "            4.91800457e-02, 1.58867136e-01]],\n",
       "  \n",
       "          [[5.04067987e-02, 1.23875260e-01, 2.67209411e-01,\n",
       "            3.32147330e-02, 9.22726020e-02, 5.45593947e-02,\n",
       "            2.39674941e-01, 1.38786897e-01],\n",
       "           [2.09324419e-01, 4.84275408e-02, 5.96491992e-02,\n",
       "            2.25141972e-01, 2.27852091e-02, 1.58361062e-01,\n",
       "            1.97515130e-01, 7.87954107e-02],\n",
       "           [1.44646734e-01, 1.64134949e-02, 7.79286399e-02,\n",
       "            3.63341898e-01, 9.31683928e-02, 2.86419541e-01,\n",
       "            3.29517294e-03, 1.47860814e-02],\n",
       "           [2.84416586e-01, 4.04777862e-02, 4.95781079e-02,\n",
       "            2.92821348e-01, 2.58029383e-02, 2.42619619e-01,\n",
       "            4.28059250e-02, 2.14777384e-02],\n",
       "           [2.10121945e-01, 4.39762510e-02, 2.79368088e-02,\n",
       "            1.94741488e-01, 1.22890575e-02, 4.18911994e-01,\n",
       "            5.60314432e-02, 3.59910503e-02],\n",
       "           [1.43744290e-01, 8.77014827e-03, 1.30093014e-02,\n",
       "            2.53059026e-02, 2.53891014e-02, 7.58008599e-01,\n",
       "            1.41420299e-02, 1.16306590e-02],\n",
       "           [8.59035924e-02, 3.47313983e-03, 1.27210114e-02,\n",
       "            9.74148214e-02, 4.13343832e-02, 6.52232468e-01,\n",
       "            9.95404869e-02, 7.38014048e-03],\n",
       "           [1.41907617e-01, 1.25515517e-02, 3.05090528e-02,\n",
       "            6.87104583e-01, 1.15997847e-02, 9.50046331e-02,\n",
       "            1.05340555e-02, 1.07886409e-02]],\n",
       "  \n",
       "          [[8.06367472e-02, 1.19297449e-02, 1.54215530e-01,\n",
       "            7.02673197e-03, 1.68681040e-01, 3.26600015e-01,\n",
       "            7.72162452e-02, 1.73693880e-01],\n",
       "           [1.59114137e-01, 4.36495654e-02, 1.12392046e-01,\n",
       "            3.54580730e-02, 3.50016430e-02, 3.76014352e-01,\n",
       "            5.33946380e-02, 1.84975535e-01],\n",
       "           [2.24642400e-02, 1.17649988e-03, 1.33451144e-03,\n",
       "            9.67274845e-01, 4.16398980e-04, 3.15336301e-03,\n",
       "            2.01515201e-03, 2.16502277e-03],\n",
       "           [1.30775526e-01, 1.81042477e-02, 1.04565158e-01,\n",
       "            7.82574620e-03, 3.01781353e-02, 1.05225526e-01,\n",
       "            5.14843225e-01, 8.84825066e-02],\n",
       "           [1.56727806e-01, 4.51585390e-02, 3.16332206e-02,\n",
       "            4.59056020e-01, 2.05615219e-02, 1.75943002e-01,\n",
       "            1.42385541e-02, 9.66813713e-02],\n",
       "           [1.98746875e-01, 4.70174551e-02, 4.60280627e-02,\n",
       "            4.32241291e-01, 1.35189584e-02, 1.14304073e-01,\n",
       "            5.30430749e-02, 9.51002240e-02],\n",
       "           [9.72224697e-02, 2.20545128e-01, 5.89473210e-02,\n",
       "            1.32138312e-01, 3.17302868e-02, 2.02378213e-01,\n",
       "            2.37631369e-02, 2.33275145e-01],\n",
       "           [5.61364032e-02, 1.28560411e-02, 4.05976409e-03,\n",
       "            9.05604959e-01, 7.01561978e-04, 1.09298229e-02,\n",
       "            4.57727024e-03, 5.13421325e-03]],\n",
       "  \n",
       "          [[1.11420661e-01, 4.93985005e-02, 1.01222597e-01,\n",
       "            1.40497442e-02, 1.28397182e-01, 2.80272007e-01,\n",
       "            2.72236228e-01, 4.30030823e-02],\n",
       "           [1.31258607e-01, 7.29775056e-02, 6.92634359e-02,\n",
       "            3.76371779e-02, 1.32582814e-01, 3.71336341e-01,\n",
       "            9.74409059e-02, 8.75032395e-02],\n",
       "           [1.17006369e-01, 5.08502237e-02, 7.50605389e-02,\n",
       "            4.43930954e-01, 4.28385101e-02, 1.64789811e-01,\n",
       "            7.51962885e-02, 3.03273089e-02],\n",
       "           [4.93078008e-02, 5.96116669e-02, 3.57019603e-02,\n",
       "            3.21154413e-03, 1.61394119e-01, 4.23417389e-01,\n",
       "            1.44476622e-01, 1.22878917e-01],\n",
       "           [5.52920178e-02, 2.02482957e-02, 4.33961824e-02,\n",
       "            7.94727445e-01, 1.23342900e-02, 2.41232757e-02,\n",
       "            4.17145975e-02, 8.16392433e-03],\n",
       "           [5.05089909e-02, 2.56126747e-02, 9.48764235e-02,\n",
       "            5.62258184e-01, 4.44485992e-02, 5.83942458e-02,\n",
       "            1.29363477e-01, 3.45374532e-02],\n",
       "           [1.65549710e-01, 1.55163422e-01, 1.23801179e-01,\n",
       "            1.25729382e-01, 9.69279259e-02, 9.65563729e-02,\n",
       "            1.17676578e-01, 1.18595384e-01],\n",
       "           [1.56563461e-01, 5.53783439e-02, 1.13947615e-01,\n",
       "            2.81934053e-01, 7.64849037e-02, 1.81371421e-01,\n",
       "            7.49778673e-02, 5.93423508e-02]],\n",
       "  \n",
       "          [[2.64868945e-01, 8.65098238e-02, 1.25346154e-01,\n",
       "            6.03444874e-02, 1.05708122e-01, 7.55925477e-02,\n",
       "            1.78668171e-01, 1.02961704e-01],\n",
       "           [3.69083434e-01, 9.35487077e-02, 7.40190223e-02,\n",
       "            1.93015561e-01, 4.82602119e-02, 5.11755124e-02,\n",
       "            1.05023906e-01, 6.58737123e-02],\n",
       "           [1.66620567e-01, 2.06408828e-01, 7.27532506e-02,\n",
       "            4.26143527e-01, 9.31735989e-03, 4.42266017e-02,\n",
       "            1.65156517e-02, 5.80142215e-02],\n",
       "           [2.70377129e-01, 3.10547590e-01, 1.53749064e-01,\n",
       "            4.93529104e-02, 5.04094325e-02, 6.01030663e-02,\n",
       "            2.53710411e-02, 8.00898224e-02],\n",
       "           [5.14011793e-02, 2.71816343e-01, 6.50672019e-02,\n",
       "            4.62596178e-01, 7.42175011e-03, 1.02545209e-01,\n",
       "            2.99038924e-03, 3.61617208e-02],\n",
       "           [7.15054125e-02, 3.10446322e-01, 8.26833621e-02,\n",
       "            2.41766274e-01, 3.26509215e-02, 1.92559719e-01,\n",
       "            1.83789581e-02, 5.00090457e-02],\n",
       "           [2.00268487e-03, 1.70752895e-03, 5.93517965e-04,\n",
       "            1.04436800e-02, 1.28925279e-01, 7.91331649e-01,\n",
       "            6.35838434e-02, 1.41172390e-03],\n",
       "           [1.29223570e-01, 2.29667038e-01, 1.07225589e-01,\n",
       "            4.13737863e-01, 1.24281794e-02, 2.16269586e-02,\n",
       "            1.61887612e-02, 6.99020103e-02]],\n",
       "  \n",
       "          [[2.23098680e-01, 9.32109654e-02, 6.77754581e-02,\n",
       "            1.27052709e-01, 7.59934485e-02, 2.95300066e-01,\n",
       "            4.93955240e-02, 6.81731179e-02],\n",
       "           [1.32690147e-01, 1.02260150e-01, 1.60691604e-01,\n",
       "            6.45337328e-02, 7.10559115e-02, 4.01096433e-01,\n",
       "            2.21137907e-02, 4.55582030e-02],\n",
       "           [3.44694287e-01, 2.54966281e-02, 4.62023877e-02,\n",
       "            5.32723740e-02, 1.13363259e-01, 3.45237434e-01,\n",
       "            2.03056391e-02, 5.14279865e-02],\n",
       "           [1.73108533e-01, 1.15011958e-02, 3.20055872e-01,\n",
       "            6.82659000e-02, 1.77793711e-01, 1.88983917e-01,\n",
       "            3.02162976e-03, 5.72692715e-02],\n",
       "           [2.72150666e-01, 2.97010615e-02, 1.18862838e-01,\n",
       "            8.02361667e-02, 1.62846386e-01, 2.48634815e-01,\n",
       "            1.18942354e-02, 7.56738409e-02],\n",
       "           [2.27989033e-01, 1.90845761e-03, 2.38405123e-01,\n",
       "            5.26869334e-02, 1.77167237e-01, 2.66095042e-01,\n",
       "            1.35821814e-03, 3.43899019e-02],\n",
       "           [2.11293653e-01, 2.87696328e-02, 1.42788544e-01,\n",
       "            3.21519934e-02, 5.19015640e-02, 4.68332618e-01,\n",
       "            2.33705491e-02, 4.13915068e-02],\n",
       "           [1.51780665e-01, 1.15085125e-01, 1.56276122e-01,\n",
       "            9.38496888e-02, 9.24942344e-02, 2.81487197e-01,\n",
       "            4.30189036e-02, 6.60081208e-02]],\n",
       "  \n",
       "          [[1.85471386e-01, 1.46488830e-01, 1.08539276e-01,\n",
       "            7.62821659e-02, 1.24780707e-01, 9.96927321e-02,\n",
       "            1.18387848e-01, 1.40357032e-01],\n",
       "           [5.27492650e-02, 3.27826589e-02, 1.13264702e-01,\n",
       "            1.31408930e-01, 2.41588041e-01, 3.26338083e-01,\n",
       "            1.94472298e-02, 8.24211091e-02],\n",
       "           [3.53881150e-01, 1.28690094e-01, 7.23058358e-02,\n",
       "            7.94667304e-02, 1.82724446e-02, 6.05394281e-02,\n",
       "            1.37940437e-01, 1.48903847e-01],\n",
       "           [1.55623674e-01, 3.11519891e-01, 9.24056023e-02,\n",
       "            4.43789214e-02, 1.20994046e-01, 8.29390213e-02,\n",
       "            1.14367269e-01, 7.77715892e-02],\n",
       "           [5.23546636e-01, 6.96558282e-02, 7.77614191e-02,\n",
       "            1.40136266e-02, 3.12132053e-02, 2.90368479e-02,\n",
       "            1.30505607e-01, 1.24266893e-01],\n",
       "           [5.46984553e-01, 1.27712458e-01, 4.32800576e-02,\n",
       "            3.23440954e-02, 4.78263432e-03, 1.95080042e-02,\n",
       "            4.16924618e-02, 1.83695763e-01],\n",
       "           [1.21680334e-01, 4.75462615e-01, 8.49541053e-02,\n",
       "            5.83978072e-02, 5.47875501e-02, 3.79321389e-02,\n",
       "            1.77823901e-02, 1.49002999e-01],\n",
       "           [2.80795157e-01, 1.65710881e-01, 1.14431724e-01,\n",
       "            6.57010749e-02, 6.89788461e-02, 9.43645835e-02,\n",
       "            3.34800445e-02, 1.76537707e-01]],\n",
       "  \n",
       "          [[1.11787938e-01, 1.37524782e-02, 3.87045369e-02,\n",
       "            6.19081736e-01, 1.32076526e-02, 8.44636634e-02,\n",
       "            7.29254112e-02, 4.60766405e-02],\n",
       "           [1.20101616e-01, 2.66346876e-02, 7.90349916e-02,\n",
       "            4.69915479e-01, 6.22118004e-02, 6.82833940e-02,\n",
       "            3.60478871e-02, 1.37770146e-01],\n",
       "           [2.36476600e-01, 3.01921498e-02, 1.44410580e-01,\n",
       "            1.36152610e-01, 3.46264467e-02, 8.14369842e-02,\n",
       "            1.26882821e-01, 2.09821790e-01],\n",
       "           [1.67351514e-01, 5.94387800e-02, 9.82468054e-02,\n",
       "            1.32827535e-01, 9.02495906e-02, 2.25125268e-01,\n",
       "            1.26849905e-01, 9.99105945e-02],\n",
       "           [2.60829389e-01, 2.51359735e-02, 2.21233130e-01,\n",
       "            2.12591663e-02, 2.78171357e-02, 5.55196814e-02,\n",
       "            1.70211509e-01, 2.17993990e-01],\n",
       "           [1.65299833e-01, 6.18148893e-02, 1.11632079e-01,\n",
       "            1.34551758e-02, 2.78199352e-02, 7.56676570e-02,\n",
       "            4.31164533e-01, 1.13145836e-01],\n",
       "           [3.10053319e-01, 2.71340832e-02, 1.06777690e-01,\n",
       "            6.11537844e-02, 1.36206755e-02, 5.86164072e-02,\n",
       "            2.45274112e-01, 1.77369967e-01],\n",
       "           [1.72285751e-01, 4.00909297e-02, 2.77025789e-01,\n",
       "            3.38464640e-02, 2.62141451e-02, 2.11283471e-02,\n",
       "            1.69214875e-01, 2.60193676e-01]]]], dtype=float32)>],\n",
       " [<tf.Tensor: shape=(1, 8, 8, 4), dtype=float32, numpy=\n",
       "  array([[[[0.17281744, 0.19240059, 0.26632434, 0.36845765],\n",
       "           [0.05177003, 0.17124826, 0.25412816, 0.52285355],\n",
       "           [0.08757561, 0.2048503 , 0.27354306, 0.43403098],\n",
       "           [0.16119657, 0.19616193, 0.28276998, 0.35987145],\n",
       "           [0.28749448, 0.19779211, 0.26854777, 0.24616565],\n",
       "           [0.23725998, 0.18970624, 0.27779335, 0.29524037],\n",
       "           [0.15306301, 0.19036183, 0.2834324 , 0.37314275],\n",
       "           [0.19660264, 0.18127678, 0.24863352, 0.37348703]],\n",
       "  \n",
       "          [[0.0987054 , 0.20456633, 0.39572516, 0.3010031 ],\n",
       "           [0.13223092, 0.1810597 , 0.41533348, 0.27137592],\n",
       "           [0.11699426, 0.3518146 , 0.22254525, 0.30864593],\n",
       "           [0.06908688, 0.25671956, 0.3322305 , 0.341963  ],\n",
       "           [0.02253289, 0.44256747, 0.20065328, 0.33424637],\n",
       "           [0.05798734, 0.23808675, 0.39564186, 0.30828407],\n",
       "           [0.08716759, 0.2953838 , 0.31162664, 0.30582199],\n",
       "           [0.10183765, 0.25917274, 0.33558273, 0.30340686]],\n",
       "  \n",
       "          [[0.2112886 , 0.28822914, 0.27924082, 0.22124146],\n",
       "           [0.09480555, 0.19284552, 0.28560156, 0.42674738],\n",
       "           [0.2468723 , 0.42357704, 0.18668237, 0.14286819],\n",
       "           [0.09626627, 0.27138487, 0.30312148, 0.32922736],\n",
       "           [0.1344637 , 0.34687424, 0.30636793, 0.21229418],\n",
       "           [0.13673739, 0.3755934 , 0.2887347 , 0.19893447],\n",
       "           [0.1270433 , 0.2597115 , 0.26982483, 0.3434204 ],\n",
       "           [0.21863541, 0.18009733, 0.2748353 , 0.326432  ]],\n",
       "  \n",
       "          [[0.19531186, 0.23372188, 0.2673488 , 0.3036175 ],\n",
       "           [0.00452153, 0.25220442, 0.5039509 , 0.23932321],\n",
       "           [0.03724414, 0.26847452, 0.35320908, 0.34107235],\n",
       "           [0.01568378, 0.25950804, 0.4257857 , 0.29902244],\n",
       "           [0.03000812, 0.27972907, 0.4089621 , 0.28130066],\n",
       "           [0.0185186 , 0.2377668 , 0.4349893 , 0.3087253 ],\n",
       "           [0.01470484, 0.2701384 , 0.46206057, 0.25309622],\n",
       "           [0.01793718, 0.31059843, 0.3935767 , 0.2778877 ]],\n",
       "  \n",
       "          [[0.7441104 , 0.08743229, 0.08936755, 0.0790897 ],\n",
       "           [0.04742323, 0.12849522, 0.31479383, 0.5092878 ],\n",
       "           [0.37372613, 0.12484491, 0.2700484 , 0.23138057],\n",
       "           [0.21351625, 0.23148799, 0.29534867, 0.259647  ],\n",
       "           [0.11591532, 0.22665097, 0.29557854, 0.36185512],\n",
       "           [0.0206285 , 0.22141455, 0.27570006, 0.4822569 ],\n",
       "           [0.47435018, 0.1695292 , 0.21091118, 0.14520939],\n",
       "           [0.2233255 , 0.10614819, 0.32218668, 0.34833965]],\n",
       "  \n",
       "          [[0.2723703 , 0.24446653, 0.28334764, 0.19981545],\n",
       "           [0.02174898, 0.27205136, 0.27182114, 0.4343785 ],\n",
       "           [0.05805418, 0.37140083, 0.29746908, 0.2730758 ],\n",
       "           [0.11364838, 0.352209  , 0.28669617, 0.2474465 ],\n",
       "           [0.09961332, 0.31380197, 0.26290923, 0.3236754 ],\n",
       "           [0.02143563, 0.33040634, 0.25029978, 0.3978582 ],\n",
       "           [0.05565411, 0.30452287, 0.39676458, 0.2430584 ],\n",
       "           [0.04650403, 0.2842915 , 0.34914908, 0.3200554 ]],\n",
       "  \n",
       "          [[0.08335113, 0.24983671, 0.2989673 , 0.36784482],\n",
       "           [0.16887546, 0.24453609, 0.23962533, 0.34696314],\n",
       "           [0.10185718, 0.2438134 , 0.38703996, 0.26728943],\n",
       "           [0.17937163, 0.27708197, 0.25177547, 0.29177088],\n",
       "           [0.05805307, 0.16397911, 0.33731687, 0.440651  ],\n",
       "           [0.12685034, 0.2824542 , 0.2991798 , 0.29151565],\n",
       "           [0.11837852, 0.2105398 , 0.30586678, 0.36521494],\n",
       "           [0.05517887, 0.15930049, 0.3898953 , 0.39562538]],\n",
       "  \n",
       "          [[0.4036244 , 0.17389165, 0.266115  , 0.15636894],\n",
       "           [0.02910079, 0.13721277, 0.28054622, 0.55314016],\n",
       "           [0.0797484 , 0.20676225, 0.2204348 , 0.49305454],\n",
       "           [0.02909196, 0.16478643, 0.22663756, 0.579484  ],\n",
       "           [0.03447549, 0.13543488, 0.21862768, 0.611462  ],\n",
       "           [0.01734918, 0.10728115, 0.17312993, 0.7022397 ],\n",
       "           [0.17697057, 0.22832273, 0.2678878 , 0.32681894],\n",
       "           [0.30789486, 0.23654613, 0.24584284, 0.20971611]]]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 4), dtype=float32, numpy=\n",
       "  array([[[[0.5717032 , 0.14317118, 0.13342513, 0.15170048],\n",
       "           [0.02635669, 0.19380023, 0.35543132, 0.42441177],\n",
       "           [0.04525849, 0.26610535, 0.26539618, 0.42324004],\n",
       "           [0.02055793, 0.17255181, 0.37673643, 0.4301538 ],\n",
       "           [0.01473645, 0.18451118, 0.32216606, 0.4785863 ],\n",
       "           [0.0525462 , 0.22103938, 0.3613614 , 0.36505306],\n",
       "           [0.5477364 , 0.1461644 , 0.1732176 , 0.1328816 ],\n",
       "           [0.09091256, 0.21155252, 0.30234808, 0.39518684]],\n",
       "  \n",
       "          [[0.1902222 , 0.20743659, 0.28004196, 0.3222993 ],\n",
       "           [0.01329894, 0.2237664 , 0.2494083 , 0.5135263 ],\n",
       "           [0.20835032, 0.25331616, 0.25679317, 0.28154033],\n",
       "           [0.2422611 , 0.23871891, 0.24963579, 0.26938424],\n",
       "           [0.00404798, 0.20563346, 0.22250152, 0.5678171 ],\n",
       "           [0.00579045, 0.25803706, 0.21174937, 0.5244231 ],\n",
       "           [0.04869886, 0.27852714, 0.24326847, 0.42950553],\n",
       "           [0.02767462, 0.20019329, 0.274599  , 0.49753302]],\n",
       "  \n",
       "          [[0.16218577, 0.1919406 , 0.38051796, 0.2653557 ],\n",
       "           [0.15043113, 0.19089288, 0.42847487, 0.23020113],\n",
       "           [0.0178174 , 0.20404048, 0.36249807, 0.41564396],\n",
       "           [0.05675332, 0.2712669 , 0.29382342, 0.37815633],\n",
       "           [0.02801125, 0.31383854, 0.27346116, 0.38468906],\n",
       "           [0.02529864, 0.4071429 , 0.16873278, 0.39882568],\n",
       "           [0.18504316, 0.31578016, 0.22857359, 0.27060306],\n",
       "           [0.05307476, 0.26268673, 0.3600102 , 0.32422832]],\n",
       "  \n",
       "          [[0.30269322, 0.3236147 , 0.15847348, 0.21521859],\n",
       "           [0.10550859, 0.4324053 , 0.25128764, 0.21079847],\n",
       "           [0.03574764, 0.38463032, 0.25955263, 0.32006937],\n",
       "           [0.16850838, 0.52071077, 0.14437293, 0.16640788],\n",
       "           [0.10046145, 0.37921727, 0.24109544, 0.2792259 ],\n",
       "           [0.18247874, 0.47451678, 0.17019653, 0.17280795],\n",
       "           [0.23334397, 0.3642621 , 0.23032181, 0.17207213],\n",
       "           [0.08384914, 0.4906535 , 0.21062355, 0.21487384]],\n",
       "  \n",
       "          [[0.10484292, 0.23962931, 0.27441347, 0.38111424],\n",
       "           [0.39415556, 0.20668957, 0.24922898, 0.14992586],\n",
       "           [0.08027375, 0.20560765, 0.3375994 , 0.37651923],\n",
       "           [0.04577925, 0.1900447 , 0.12938102, 0.63479495],\n",
       "           [0.02645723, 0.17377146, 0.23515688, 0.5646145 ],\n",
       "           [0.05965369, 0.19742715, 0.16699605, 0.5759231 ],\n",
       "           [0.05502283, 0.23547833, 0.2961366 , 0.41336223],\n",
       "           [0.42463955, 0.19255532, 0.22810586, 0.15469927]],\n",
       "  \n",
       "          [[0.13820656, 0.289291  , 0.21531901, 0.35718343],\n",
       "           [0.04438661, 0.22289556, 0.2185401 , 0.51417774],\n",
       "           [0.40973318, 0.32319266, 0.16086587, 0.10620827],\n",
       "           [0.3108651 , 0.36682367, 0.18506433, 0.13724692],\n",
       "           [0.07555059, 0.26067185, 0.30223766, 0.36153984],\n",
       "           [0.11199783, 0.3878927 , 0.22299266, 0.27711675],\n",
       "           [0.22167695, 0.31649157, 0.22015372, 0.24167776],\n",
       "           [0.5223914 , 0.11702476, 0.16826737, 0.1923165 ]],\n",
       "  \n",
       "          [[0.16778983, 0.3040305 , 0.23532419, 0.2928555 ],\n",
       "           [0.02751758, 0.19441077, 0.33935967, 0.43871197],\n",
       "           [0.11213055, 0.2561939 , 0.3108761 , 0.32079944],\n",
       "           [0.03054923, 0.20515881, 0.3605699 , 0.40372205],\n",
       "           [0.27803364, 0.26670364, 0.22117743, 0.23408532],\n",
       "           [0.22448373, 0.25178224, 0.27946562, 0.24426846],\n",
       "           [0.10763472, 0.37220827, 0.23103376, 0.28912327],\n",
       "           [0.08437016, 0.30083767, 0.33377394, 0.2810183 ]],\n",
       "  \n",
       "          [[0.17113084, 0.1492767 , 0.33029172, 0.34930074],\n",
       "           [0.02856153, 0.14207318, 0.38487583, 0.44448948],\n",
       "           [0.09100264, 0.142185  , 0.35342544, 0.41338688],\n",
       "           [0.06730795, 0.24575526, 0.31854272, 0.36839408],\n",
       "           [0.41465068, 0.31054175, 0.15713075, 0.11767684],\n",
       "           [0.18195306, 0.432391  , 0.210703  , 0.17495303],\n",
       "           [0.08387069, 0.2317404 , 0.30296922, 0.38141972],\n",
       "           [0.10365738, 0.14912573, 0.32450634, 0.4227106 ]]]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 4), dtype=float32, numpy=\n",
       "  array([[[[2.47981492e-02, 1.37770772e-01, 4.79686826e-01,\n",
       "            3.57744247e-01],\n",
       "           [1.19538352e-01, 2.58721828e-01, 3.83556157e-01,\n",
       "            2.38183737e-01],\n",
       "           [1.74941227e-03, 5.18065169e-02, 4.21690702e-01,\n",
       "            5.24753392e-01],\n",
       "           [6.03282079e-02, 2.32976168e-01, 3.88346732e-01,\n",
       "            3.18348914e-01],\n",
       "           [3.81785398e-03, 1.04439802e-01, 3.06935936e-01,\n",
       "            5.84806383e-01],\n",
       "           [1.20141869e-02, 1.57977149e-01, 4.13478851e-01,\n",
       "            4.16529804e-01],\n",
       "           [4.06447649e-02, 1.61688775e-01, 4.11910236e-01,\n",
       "            3.85756195e-01],\n",
       "           [3.67902592e-03, 8.06295946e-02, 3.24374855e-01,\n",
       "            5.91316462e-01]],\n",
       "  \n",
       "          [[3.46482038e-01, 1.34411260e-01, 3.41642380e-01,\n",
       "            1.77464336e-01],\n",
       "           [4.68277752e-01, 1.16595946e-01, 2.32308775e-01,\n",
       "            1.82817549e-01],\n",
       "           [4.26583052e-01, 2.00523451e-01, 2.15127259e-01,\n",
       "            1.57766268e-01],\n",
       "           [5.82726300e-01, 9.38319862e-02, 1.47145599e-01,\n",
       "            1.76296130e-01],\n",
       "           [4.18457389e-01, 1.93541557e-01, 2.08136499e-01,\n",
       "            1.79864496e-01],\n",
       "           [5.11723280e-01, 1.01698369e-01, 1.63342610e-01,\n",
       "            2.23235756e-01],\n",
       "           [9.05440673e-02, 2.35124990e-01, 3.62660527e-01,\n",
       "            3.11670333e-01],\n",
       "           [5.81695199e-01, 8.74585584e-02, 2.04091892e-01,\n",
       "            1.26754344e-01]],\n",
       "  \n",
       "          [[5.47006205e-02, 2.01410905e-01, 3.07405323e-01,\n",
       "            4.36483145e-01],\n",
       "           [1.35625852e-02, 1.73058689e-01, 3.18837345e-01,\n",
       "            4.94541466e-01],\n",
       "           [2.43735723e-02, 1.95483476e-01, 3.03752333e-01,\n",
       "            4.76390630e-01],\n",
       "           [1.15323707e-01, 2.32218429e-01, 3.45393062e-01,\n",
       "            3.07064801e-01],\n",
       "           [1.93069857e-02, 1.46876663e-01, 2.92439491e-01,\n",
       "            5.41376889e-01],\n",
       "           [1.97697785e-02, 1.81765169e-01, 3.65597844e-01,\n",
       "            4.32867199e-01],\n",
       "           [4.59243357e-02, 1.97783262e-01, 3.64354432e-01,\n",
       "            3.91937941e-01],\n",
       "           [1.42012853e-02, 1.67081028e-01, 2.80674249e-01,\n",
       "            5.38043439e-01]],\n",
       "  \n",
       "          [[1.61476195e-01, 2.46985182e-01, 2.20105469e-01,\n",
       "            3.71433139e-01],\n",
       "           [1.57935038e-01, 1.78398356e-01, 2.26681024e-01,\n",
       "            4.36985612e-01],\n",
       "           [7.75068253e-02, 1.59921110e-01, 2.21094131e-01,\n",
       "            5.41477978e-01],\n",
       "           [6.52007014e-02, 1.88234761e-01, 3.04997712e-01,\n",
       "            4.41566765e-01],\n",
       "           [1.59749426e-02, 9.54935029e-02, 1.70437202e-01,\n",
       "            7.18094409e-01],\n",
       "           [1.28323501e-02, 8.79520774e-02, 1.86686546e-01,\n",
       "            7.12529063e-01],\n",
       "           [8.51393268e-02, 1.52641177e-01, 2.49931157e-01,\n",
       "            5.12288392e-01],\n",
       "           [5.02915680e-02, 1.25643224e-01, 2.18966410e-01,\n",
       "            6.05098903e-01]],\n",
       "  \n",
       "          [[3.45895082e-01, 2.44793609e-01, 2.01137662e-01,\n",
       "            2.08173648e-01],\n",
       "           [3.01856160e-01, 2.17387006e-01, 1.99652925e-01,\n",
       "            2.81103939e-01],\n",
       "           [1.78083241e-01, 3.46175164e-01, 2.47718126e-01,\n",
       "            2.28023455e-01],\n",
       "           [1.30334765e-01, 2.98639238e-01, 2.79708356e-01,\n",
       "            2.91317582e-01],\n",
       "           [1.67510733e-01, 4.83942658e-01, 2.20636114e-01,\n",
       "            1.27910540e-01],\n",
       "           [9.38935429e-02, 4.55906570e-01, 2.55066752e-01,\n",
       "            1.95133135e-01],\n",
       "           [8.68342817e-02, 3.00470173e-01, 3.03414822e-01,\n",
       "            3.09280723e-01],\n",
       "           [3.86092693e-01, 2.48286411e-01, 1.59947053e-01,\n",
       "            2.05673903e-01]],\n",
       "  \n",
       "          [[8.16073716e-02, 1.88830927e-01, 2.18497500e-01,\n",
       "            5.11064172e-01],\n",
       "           [1.34996176e-02, 1.54616699e-01, 2.27641419e-01,\n",
       "            6.04242265e-01],\n",
       "           [2.69716978e-02, 2.09708199e-01, 2.61902034e-01,\n",
       "            5.01418114e-01],\n",
       "           [1.04548961e-01, 2.07654968e-01, 2.70353407e-01,\n",
       "            4.17442620e-01],\n",
       "           [4.26214421e-03, 1.80430293e-01, 2.12800324e-01,\n",
       "            6.02507234e-01],\n",
       "           [5.12451343e-02, 1.54106826e-01, 2.31604844e-01,\n",
       "            5.63043177e-01],\n",
       "           [2.23889258e-02, 3.32157195e-01, 2.51352698e-01,\n",
       "            3.94101113e-01],\n",
       "           [2.58543342e-02, 2.27932721e-01, 2.12001681e-01,\n",
       "            5.34211278e-01]],\n",
       "  \n",
       "          [[1.82502404e-01, 2.78417408e-01, 2.66964674e-01,\n",
       "            2.72115529e-01],\n",
       "           [8.00652206e-02, 2.25985959e-01, 3.46486598e-01,\n",
       "            3.47462207e-01],\n",
       "           [6.52886170e-04, 9.17904601e-02, 2.74379075e-01,\n",
       "            6.33177519e-01],\n",
       "           [1.05558420e-02, 1.66434109e-01, 2.61573046e-01,\n",
       "            5.61437011e-01],\n",
       "           [1.00123254e-03, 9.90451723e-02, 1.85444444e-01,\n",
       "            7.14509130e-01],\n",
       "           [2.26811552e-03, 1.40319079e-01, 2.15607285e-01,\n",
       "            6.41805589e-01],\n",
       "           [2.42600024e-01, 1.98973909e-01, 2.54290998e-01,\n",
       "            3.04135054e-01],\n",
       "           [1.35300132e-02, 1.38607830e-01, 2.68551320e-01,\n",
       "            5.79310894e-01]],\n",
       "  \n",
       "          [[1.52806655e-01, 2.13655666e-01, 3.49391371e-01,\n",
       "            2.84146339e-01],\n",
       "           [5.56990132e-02, 1.97871655e-01, 3.10772181e-01,\n",
       "            4.35657203e-01],\n",
       "           [3.21896374e-02, 3.79174978e-01, 3.19050461e-01,\n",
       "            2.69584864e-01],\n",
       "           [5.01752757e-02, 2.57732362e-01, 4.03370142e-01,\n",
       "            2.88722247e-01],\n",
       "           [3.09512205e-02, 3.92893553e-01, 3.13144416e-01,\n",
       "            2.63010770e-01],\n",
       "           [2.90618371e-02, 3.59590203e-01, 3.54118258e-01,\n",
       "            2.57229745e-01],\n",
       "           [9.95532498e-02, 4.02875662e-01, 2.38545910e-01,\n",
       "            2.59025186e-01],\n",
       "           [3.89884599e-02, 3.56535345e-01, 2.98120201e-01,\n",
       "            3.06355983e-01]]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 8, 8, 4), dtype=float32, numpy=\n",
       "  array([[[[0.22336452, 0.18801099, 0.27054417, 0.31808034],\n",
       "           [0.18412967, 0.11659009, 0.31254193, 0.3867383 ],\n",
       "           [0.02303739, 0.10819165, 0.3974781 , 0.47129288],\n",
       "           [0.03509511, 0.20625128, 0.34179112, 0.4168625 ],\n",
       "           [0.05892419, 0.09034682, 0.36529657, 0.48543242],\n",
       "           [0.02662653, 0.09953088, 0.33249187, 0.5413507 ],\n",
       "           [0.16304085, 0.1342292 , 0.34210056, 0.36062935],\n",
       "           [0.050781  , 0.09574018, 0.36056504, 0.49291378]],\n",
       "  \n",
       "          [[0.68084335, 0.10308053, 0.12208322, 0.09399285],\n",
       "           [0.08083148, 0.17050111, 0.29675776, 0.4519097 ],\n",
       "           [0.00963369, 0.13266721, 0.21684992, 0.6408492 ],\n",
       "           [0.20120521, 0.26368764, 0.28299496, 0.25211212],\n",
       "           [0.01086538, 0.14398882, 0.225692  , 0.61945385],\n",
       "           [0.01803166, 0.17155598, 0.27486172, 0.53555065],\n",
       "           [0.0746146 , 0.24771062, 0.26438326, 0.41329154],\n",
       "           [0.04638205, 0.14753155, 0.25366905, 0.55241734]],\n",
       "  \n",
       "          [[0.13728139, 0.20240828, 0.3005425 , 0.3597678 ],\n",
       "           [0.07571133, 0.1742963 , 0.23621933, 0.513773  ],\n",
       "           [0.00525123, 0.14319275, 0.2946081 , 0.556948  ],\n",
       "           [0.03370417, 0.14181809, 0.2974886 , 0.52698916],\n",
       "           [0.00841067, 0.1642427 , 0.25057495, 0.5767716 ],\n",
       "           [0.00534803, 0.12127907, 0.2549427 , 0.6184302 ],\n",
       "           [0.08074268, 0.20228685, 0.27250978, 0.4444607 ],\n",
       "           [0.00890859, 0.1764995 , 0.29297838, 0.52161354]],\n",
       "  \n",
       "          [[0.3183559 , 0.2589701 , 0.18125305, 0.241421  ],\n",
       "           [0.02766089, 0.25771347, 0.1888367 , 0.52578896],\n",
       "           [0.02690182, 0.25718135, 0.24621317, 0.46970367],\n",
       "           [0.04305324, 0.26841092, 0.27079856, 0.4177373 ],\n",
       "           [0.02727513, 0.223449  , 0.26306742, 0.4862084 ],\n",
       "           [0.01231604, 0.17810875, 0.25998262, 0.5495927 ],\n",
       "           [0.22741288, 0.2752234 , 0.20036463, 0.2969991 ],\n",
       "           [0.02019808, 0.25667712, 0.21646023, 0.5066645 ]],\n",
       "  \n",
       "          [[0.06186595, 0.2611008 , 0.30748236, 0.36955085],\n",
       "           [0.03335448, 0.23751798, 0.29347005, 0.43565756],\n",
       "           [0.00673771, 0.30301484, 0.29074022, 0.39950722],\n",
       "           [0.17199922, 0.24956219, 0.31156948, 0.26686913],\n",
       "           [0.01617168, 0.38263503, 0.2256967 , 0.3754966 ],\n",
       "           [0.00405338, 0.36827418, 0.23331133, 0.3943611 ],\n",
       "           [0.0080662 , 0.23679312, 0.28839648, 0.46674427],\n",
       "           [0.00687716, 0.28970584, 0.24987446, 0.4535425 ]],\n",
       "  \n",
       "          [[0.273731  , 0.19561027, 0.25237712, 0.2782816 ],\n",
       "           [0.11239205, 0.2105323 , 0.30127704, 0.37579867],\n",
       "           [0.02372353, 0.2441865 , 0.24932323, 0.48276675],\n",
       "           [0.04423427, 0.2403769 , 0.22204942, 0.49333945],\n",
       "           [0.02036609, 0.26505175, 0.2506515 , 0.46393073],\n",
       "           [0.00529319, 0.19159994, 0.21200645, 0.59110045],\n",
       "           [0.04424107, 0.22185625, 0.31936494, 0.4145378 ],\n",
       "           [0.01947732, 0.23767471, 0.2606612 , 0.4821867 ]],\n",
       "  \n",
       "          [[0.37702835, 0.22052284, 0.23403937, 0.16840948],\n",
       "           [0.077347  , 0.19980286, 0.3398843 , 0.3829659 ],\n",
       "           [0.00413283, 0.10794468, 0.49482745, 0.39309502],\n",
       "           [0.2231044 , 0.33347818, 0.2205357 , 0.22288173],\n",
       "           [0.00971762, 0.1349977 , 0.37649655, 0.4787881 ],\n",
       "           [0.01880997, 0.17345436, 0.2939887 , 0.5137469 ],\n",
       "           [0.04689981, 0.23009369, 0.2987022 , 0.42430434],\n",
       "           [0.00785625, 0.09580715, 0.4226548 , 0.47368175]],\n",
       "  \n",
       "          [[0.2740235 , 0.17750938, 0.25525293, 0.29321414],\n",
       "           [0.03061916, 0.1550088 , 0.23374137, 0.5806306 ],\n",
       "           [0.01166285, 0.22515444, 0.22458524, 0.53859746],\n",
       "           [0.05187892, 0.17172936, 0.2937677 , 0.48262396],\n",
       "           [0.00495203, 0.17300084, 0.22225145, 0.5997957 ],\n",
       "           [0.00306029, 0.22312635, 0.22210059, 0.5517128 ],\n",
       "           [0.03064333, 0.1171359 , 0.20846738, 0.6437534 ],\n",
       "           [0.01404999, 0.1541615 , 0.23376377, 0.5980247 ]]]],\n",
       "        dtype=float32)>],\n",
       " ['church', 'service', 'is', 'good'],\n",
       " ['egbegbe', 'ha', 'wɔ', 'akpa', 'vevi', 'a', 'e', '<end>'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('church service is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x2c361e351f0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101,    1,  272, ...,    0,    0,    0],\n",
       "       [  21, 1727,    4, ...,    0,    0,    0],\n",
       "       [  25, 2656,  945, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  20,    1,   37, ...,    0,    0,    0],\n",
       "       [ 222,    3, 3098, ...,    0,    0,    0],\n",
       "       [2400,    3, 3098, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ewe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
